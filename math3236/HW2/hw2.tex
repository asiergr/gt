\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{parskip}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\renewcommand{\P}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\var}[1]{\text{var}\left[#1\right]}
\newcommand{\randsamp}{X_1,\dots,X_n}
\newcommand{\mgf}{moment generating function }
\newcommand{\clt}{central limit theorem}
\newcommand{\mle}{M.L.E}
\DeclareMathOperator*{\Binomial}{Binomial}


\title{HW2}
\author{Asier Garcia Ruiz}

\begin{document}
\maketitle

\section*{Section 8.7}
\subsection*{1}
(a)
\begin{proof}
    We have that $\randsamp$ is a random sample from the Poisson distribution
    with mean $\theta$. Given that this is a Poisson distribution we know that
    \begin{equation*}
        \var{X_i} = g(\theta) = \theta % this easy??
    \end{equation*}
    for all $i = 1,\dots,n$.
\end{proof}

(b)
\begin{proof}
    Now to find the \mle\ of $g(\theta)$ we start by defining the likelihood
    function
    \begin{equation*}
        L_n(\theta) = f(x_1; \theta)\dots f(x_n;\theta)
        = \prod_{i = 1}^n \frac{\theta^{x_i} e^{-\theta}}{x_i!}.
    \end{equation*}

    Now we take the log-likelihood function
    \begin{align*}
        LL_n(\theta) & = \ln(L_n(\theta)),                                                              \\
                     & = \sum_{i = 1}^n \ln\left(\frac{\theta^{x_i}e^{-\theta}}{x_i!}\right),           \\
                     & = \sum_{i = 1}^n \left[ \ln(\theta^{x_i}) + \ln(e^{-\theta}) - \ln(x_i!)\right], \\
                     & = \sum_{i = 1}^n \left[ x_i\ln(\theta) -\theta - \ln(x_i!)\right].               \\
    \end{align*}

    To find the minimum, we take the derivative with respect to $\theta$.
    \begin{equation*}
        \frac{dLL_n(\theta)}{d\theta}  = \sum_{i = 1}^n \left[\frac{x_i}{\theta} - 1\right]
        = \sum_{i = 1}^n\frac{x_i}{\theta} - \sum_{i = 1} ^n 1
        = -n + \frac{1}{\theta}\sum_{i = 1} ^n x_i.
    \end{equation*}

    Now we set this equal to zero and solve for $\theta$.
    \begin{equation*}
        \begin{gathered}
            -n + \frac{1}{\theta}\sum_{i = 1} ^n x_i = 0,\\
            \frac{1}{\theta}\sum_{i =1}^n x_i = n,\\
            \sum_{i =1}^n x_i = \theta n,\\
            \theta = \frac{1}{n}\sum_{i = 1}^n x_i = \bar{X}_n.
        \end{gathered}
    \end{equation*}

    It is pretty easy to show this is an unbiased estimator as we know already
    that the sample mean is an unbiased estimator. Since in the Poisson distribution
    the mean and variance are equal, this is also an unbiased estimator for the
    variance.


\end{proof}

\subsection*{Problem 7}
Additionally, use this problem to show that there is no unbiased estimator
of $p^{\frac{1}{2}}$.

\section*{7.5}
\subsection*{Problem 8}
\subsection*{Problem 11}

\section*{7.6}
\subsection*{Problem 6}
\subsection*{Problem 23}

\section*{Problem 7}
Assuming the inverse iin the method of moments exists, and is a continuously
differentiable function, show that the method of moments estimators are
asymptotically normal.

\end{document}