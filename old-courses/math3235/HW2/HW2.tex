\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{parskip}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\ra}{\rightarrow}

% Prob theory specific
\renewcommand{\P}[1]{\mathbb{P}(#1)}
\newcommand{\probspace}{(\Omega, \mathcal{F}, \mathbb{P})}
\newcommand{\Image}[1]{\text{Im}\, #1}
\newcommand{\E}[1]{\mathbb{E}(#1)}

\title{HW1}
\author{Asier Garcia Ruiz }
\date{August 2021}

\begin{document}
    \maketitle
    \section*{Exercises}
    \subsection*{2.10}
    If $E$ is an event of the probability space $\probspace$ show that
    the indicator function of $E$, defined to be the function $1_E$ on
    $\Omega$ given by
    \begin{equation*}
        1_E =\begin{cases}
            1 \quad &\text{if} \ \omega \in E \\
            0 \quad &\text{if} \ \omega \notin E \\
        \end{cases}
    \end{equation*}
    is a discrete randon variable.
    \begin{proof}
        We can see that $Im(1_E) = \{0, 1\}$, a countable subset of $\R$. Now we consider
        $\{\omega \in \Omega : 1_E(\omega) = x\}$ for $x \in \R$, we have to show this set
        is in $\F$. Now note that
        $\{\omega \in \Omega : 1_E(\omega) = 0\} = \{\omega \in \Omega: \omega \notin E\}
        = E^C \subseteq \F$ by the properties of an event space. Similarly
        $\{\omega \in \Omega : 1_E(\omega) = 1\} = \{\omega \in \Omega : \omega \in E\}
        = E \subseteq \F$. Hence, $1_E$ is a discrete random variable.
    \end{proof}
    \subsection*{2.11}
    Let $\probspace$ be a probability space in which
    $$\Omega = \{1,2,3,4,5,6\}, \qquad \F = \{\varnothing, \{2,4,6\}, \{1,3,5\}, \Omega\}$$
    and let $u, V, W$ be functions on $\Omega$ defined by
    $$U(\omega)= \omega, \qquad V(\omega) = \begin{cases}
        1 \quad &\text{if $\omega$ is even} \\
        0 \quad &\text{if $\omega$ is odd} \\
    \end{cases}, \qquad
    W(\omega)= \omega ^2$$
    for $\omega \in \Omega$. Determine which of $U, V, W$ are discrete random variables
    on the probability space.
    \begin{proof}
        We will start with $U$, clearly $\Image{U} = \Omega \subseteq \R$.
        Now, we can see that $\{\omega \in \Omega: U(\omega) = 1\} = \{1\} \not \subset \F$.
        Hence $U$ is not a discrete random variable on $\probspace$.

        Now consider $V$. Again, clearly $V(\Omega) = \{0,1\} \subset \R$.
        Now, consider $\{\omega \in \Omega: V(\omega)=0\} = \{1,3,5\} \subset \F$. We
        also see that $\{\omega \in \Omega: V(\omega)=1\} = \{2,4,6\} \subset \F$. Hence
        we have that $V$ is a discrete random variable on $\probspace$.

        Lastly, we consider $W$. Observe that $\Image{W} = \{1,4,9,16,25,36\} \subset \R$
        and is also countable. Now consider 
        $\{\omega \in \Omega: W(\omega) = 1\} = \{1\} \not \subset \F$. Hence, $W$ is not
        a random variable on $\probspace$.
    \end{proof}
    \subsection*{2.24}
    If $X$ s a discrete random variable having the geometric distribution with parameter
    $p$ show that the probability that $X$ is greater than $k$ is $(1-p)^k$.
    \begin{proof}
        We know that $X$ is geometric with parameter $p \in (0,1)$. Hence
        $\P{X=k} = pq^{k-1}$, for $k=0,1,2,...$. Now, we can write
        \begin{align*}
            \P{X > k} &= \P{X = k + 1} + \P{X = k + 2} + ... \\
                &= pq^{k} + pq^{k + 1} + ... \\
                &= pq^k(1+q^1 + ...) \\
                &= pq^k \sum_{i = 0}^{\infty}q^i \\
                &= pq^k \left(\frac{1}{1 - q}\right) \\
                &= pq^k\frac{1}{p} \\
                &= q^k = (1-p)^k
        \end{align*}
    \end{proof}
    \subsection*{3.8}
    Two cards are drawn at random from a deck of 52 cards. If $X$ denotes the number
    of aces drawn and $Y$ denotes the number of kings, display the joint mass
    functionof $X$ and $Y$ in the tabular form of Table 3.1.

    First let $N = \binom{52}{2}$.
    \begin{table}[h!]
        \begin{center}
            \begin{tabular}{r | c | c | c}
                        & $x = 0$ & $x = 1$ & $x = 2$ \\
                \hline
                $y = 0$ & $[\binom{4}{0}\binom{4}{0}\binom{44}{2}]/N$ & $[\binom{4}{1}\binom{4}{0}\binom{44}{1}]/N$ & $[\binom{4}{2}\binom{4}{0}\binom{44}{0}]/N$ \\
                \hline
                $y = 1$ & $[\binom{4}{0}\binom{4}{1}\binom{44}{1}]/N$ & $[\binom{4}{1}\binom{4}{1}\binom{44}{0}]/N$ & \\
                \hline
                $y = 2$ & $[\binom{4}{0}\binom{4}{2}\binom{44}{0}]/N$ & & \\
                
            \end{tabular}
        \end{center}
    \end{table}
    \section*{Problems}
    \subsection*{4}
    For what values of $c$ and $\alpha$ is the function $p$, defined by
    $$p(k) = \begin{cases}
        ck^\alpha & \text{for} \ k = 1,2,..., \\
        0 & \text{otherwise}
    \end{cases}$$
    a mass function?
    \begin{proof}
        We know that a mass function is a mapping $\R \ra [0,1]$. Hence, for $p$ to
        be a probability mass function then $p(k) \in [0,1]$ for $k=1,2,3,...$. 
        Additionally, we also know that $\sum_k ck^\alpha = 1$. We begin
        by writing
        \begin{align*}
            \sum_{k=1}^\infty ck^\alpha &= 1 \\
            c = \frac{1}{\sum_{k=1}^\infty k^\alpha} \\
        \end{align*}
        Hence, we need $\alpha > 1$ for $c$ to exist. Thus, we have
        a pmf.

    \end{proof}
    \subsection*{5}
    \textit{Lack-of-memory property}. If $X$ has the geometric distribution with parameter
    $p$, show that $$\P{X > m + n | X > m} = \P{X > n}$$ for $m,n=0,1,2,3,...$.
    We say that $X$ has the 'lack of memory property' since, if we are given
    $X-m > 0$, then the distribution of $X-m$ is the same as the original distribution
    of $X$. Show that thegeometric distribution is the only distribution concentrated on the
    positive integers with the lack-of-memory property.
    \begin{proof}
        We will start by simplifying
        \begin{align*}
            \P{X > m + n | X > m} &= \frac{\P{X > m + n \cap X > m}}{\P{X > m}} \\
                &= \frac{\P{X > m + n}}{\P{X > m}}
                \intertext{Now using the previous results of Exercie 2.24}
                &= \frac{(1-p)^{m + n}}{(1-p)^m} \\
                &= (1-p)^n = \P{X > n}
        \end{align*}
        Hence, we are done.

        Now we will show that the geometric distribution is the only concentrated
        distribution on the positive integers with this property. First we note
        a few facts. From the property we can see that 
        $\P{X > m+n \cap X > m} = \P{X >n}\P{X>m}$ and
        $\P{X>m+n} = \P{X>m}\P{X>n}$. We also observe that for $m,n > 0$,
        $\P{X > 2} = \P{X > 1}\P{X>1} = \P{X>1}^2$, and
        $\P{X > 3} = \P{X > 1}\P{X>2} = \P{X>1}\P{X>1}^2$. We see that
        generally $$\P{X>k} = \P{X>1}^k.$$ Thus, a lack of memory implies
        a geometric distribution.

        Now we can also see that
        \begin{align*}
        \P{X > 1} &= 1 - \P{X \leq 1} \\
            &= 1 - \P{X = 0} - \P{X=1} \\
            &= 1 - 0 - \P{X = 1} \\
            &= 1 - \P{X = 1}
        \end{align*}
        Hence, $\P{X > k} = (1-\P{X=1})^k$. Letting $p=\P{X=1}$
        and $q = 1 - p$ we get that the above equality is the same as
        $\P{X > k} * q^k$. This is equivalent to our first formula
        for the lack-of-memory property, hence a geometric distribution
        implies lack-of-memory.
    \end{proof}
    
    \subsection*{7}
    \textit{Coupon collecting problem}. There are $c$ different types of coupon,
    and each coupon obtained is equally likely to be any one of the $c$ types. Find the
    probability that the first $n$ coupons which you collect do not form a complete set,
    and deduce an expression for the mean number of coupons you will need to collect
    before you have a complete set.
    \begin{proof}
        We let $Y_i$ be the event that the first $n$ coupons collected
        do not contain coupon $i$. We get $\P{Y_i} = \left(\frac{c-1}{c}\right)^n$
        Extending this, $\P{Y_i \cap Y_j}=\left(\frac{c-2}{c}\right)^n$ is
        the probability that the first $n$ coupons do not contain $i$ and $j$.
        Generally, $$\P{A_{i_1} \cap A_{i_2} \cap A_{i_n}} = \frac{(c-k)^n}{c^n}$$.
        Hence, we are looking for the probability
        \begin{align*}
            \P{\bigcup_i A_i} &= \binom{c}{1}\frac{(c- 1)^n}{c^n} 
                                    + \binom{c}{2}\frac{(c-2)^n}{c^n}
                                    + ... + (-1)^c\binom{c}{c-1}\frac{1}{c^n}
        \end{align*}
        Here, each combination $\binom{c}{k}$ counts the number of subsets
        of size $k$ in the group of $c$ coupons.

        Now we will find the mean number of coupons you need to collect before having
        a complete set. First, we note that the probability of collecting a new
        coupon is $$p_k = \frac{n - (k-1)}{n}.$$ We will let $X$ be the times we draw before getting a complete
        set. We also let $X_i$ be the time needed to collect the $i$th coupon after
        $i-1$ coupons have been collected. Hence we have that
        $X = X_1 + X_2 + ... + X_n$. We also note that each $X_k$ is geometrically
        distributed such that $\E{X_k} = \frac{1}{p_k} = \frac{n}{n - k - 1}$.
        Now taking expectations
        \begin{align*}
            \E{X} &= \E{X_1 + X_2 + ... + X_n}
            \intertext{By linearity of expectation}
                &= \E{X_1} + \E{X_2} + ... + \E{X_n} \\
                &= \frac{1}{p_1} + \frac{1}{p_2} + ... + \frac{1}{p_n} \\
                &= \frac{n}{n} + \frac{n}{n - 1} + ... + \frac{n}{1} \\
                &= n\left(\frac{1}{1} + \frac{1}{2} + ... + \frac{1}{n}\right) \\
                &= n H_n \\
        \end{align*}
        Where $H_n$ is the $n$th harmonic number.
    \end{proof}
\end{document}