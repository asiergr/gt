\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{parskip}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\ra}{\rightarrow}

% Prob theory specific
\renewcommand{\P}[1]{\mathbb{P}(#1)}
\newcommand{\probspace}{(\Omega, \mathcal{F}, \mathbb{P})}
\newcommand{\Image}[1]{\text{Im}\, #1}
\newcommand{\E}[1]{\mathbb{E}(#1)}

\title{HW3}
\author{Asier Garcia Ruiz }
\date{August 2021}

\begin{document}
\maketitle

\section*{Exercises}
\subsection*{3.25} %DONE
If $X$ and $Y$ are independent discrete random variables, show that the two random
variables $g(X)$ and $h(Y)$ are independent also, for any functions $g$ and $h$
which map $\R$ into $\R$.

\begin{proof}
    We can write for all $a,b \in \R$
    \begin{align*}
        \P{g(X) = a, h(Y) = b} & = \sum_{x,y:g(x) = a, h(y) = b} \P{X=x,Y=y}           \\
                               & =\sum_{x,y:g(x) = a, h(y) = b} \P{X=x} \P{Y=y}        \\
                               & = \sum_{x:g(x) = a} \P{X=x} \sum_{y:h(y) = b} \P{Y=y} \\
                               & = \P{g(X) = a} \P{h(Y)= b}
    \end{align*}
    Hence, they are independent.
\end{proof}
\subsection*{3.42} %DONE
Let $N$ be the number of the events $A_1, A_2, . . . , A_n$ which occur. Show that
$$\E{N}= \sum_{i=1}^n \P{A_i}.$$

\begin{proof}
    Let $1_{N_i}$ be the indicator function of event $A_i$. Then we have that
    ${N=1_{N_1} +1_{N_2} + ... + 1_{N_n}}$. Now taking expectations
    \begin{align*}
        \E{N} & = \E{1_{N_1} +1_{N_2} + ... + 1_{N_n}}          \\
              & = \E{1_{N_1}} + \E{1_{N_2}} + ... + \E{1_{N_n}} \\
              & = \P{A_1} + \P{A_2} + ... + \P{A_n}             \\
              & = \sum_{i=1}^n \P{A_i}
    \end{align*}
\end{proof}
\subsection*{4.18} %DONE
If $X$ is a random variable with probability generating function $G_X(s)$,
and $k$ is a positive integer, show that $Y = kX$ and $Z = X + k$ have
probability generating functions.
$$G_Y(s) = G_X(s^k), \qquad G_Z(s) = s^k G_X(s).$$

\begin{proof}
    We will start by looking at $Y = kX$. Following from the definition of a
    probability generating function we have that
    \begin{align*}
        G_Y(s) & = \E{s^Y}     \\
               & = \E{s^{kX}}  \\
               & = \E{(s^k)^X} \\
               & = G_X (s^k)
    \end{align*}
    Now we will look at $Z = X + k$
    \begin{align*}
        G_Z (s) & = \E{s^Z}       \\
                & = \E{s^{X + k}} \\
                & = \E{s^k s^X}
        \intertext{Now since $s^k$ is a constant.}
                & = s^k \E{s^X}   \\
                & = s^k G_X(s)
    \end{align*}
\end{proof}
\subsection*{4.41} %DONE
Use generating functions to find the distribution of $X + Y$,
where $X$ and $Y$ are independent random variables,
$X$ having the binomial distribution with parameters $m$ and $p$,
and $Y$ having the binomial distribution with parameters $n$ and $p$.
Deduce that the sum of $n$ independent random variables, each having the Bernoulli
distribution with parameter $p$, has the binomial distribution with parameters $n$
and $p$.

\begin{proof}
    We will start by finding the probability generating function of $X+Y$.
    \begin{align*}
        G_{X+Y} & = G_X(s) G_Y(s)                                     \\
                & = \left(\sum_{k=0}^m\binom{m}{k} p^k q^{m-k}\right)
        \left(\sum_{k=0}^n \binom{n}{k} p^k q^{n-k} \right)           \\
                & =  (q + ps)^m(q + ps)^n                             \\
                & = (q + ps)^{m+n}                                    \\
                & = \sum_{k=0}^{m+n}\binom{m+n}{k}p^k q^{n+m -k}
    \end{align*}
    Now consider $n$ random variables $X_1, X_2, ..., X_n$ all with a Bernoulli
    distribution with parameter $p$. We can write the sum $X = \sum_i X_i$
    with probability generating function
    \begin{align*}
        G_X (s) & = G_{X_1 + X_2 + ... + X_n}   \\
                & = G_{X_1}G_{X_2}...G_{X_n}    \\
                & = (q + ps)(q + ps)...(q + ps) \\
                & = (q+ps)^n
    \end{align*}
\end{proof}
\section*{Problems}
\subsection*{3.4} %DONE
% help understand https://math.stackexchange.com/questions/355820/expected-value-of-the-minimum-discrete-case
Let $X_1, X_2,...,X_n$ be independent discrete random variables, each having mass function
$$ \P{X_i = k} = \frac{1}{N}, \text{ for } k = 1,2,...,N.$$
Find the mass function of $U_n$ and $V_n$, given by
$$U_n= \min\{X_1,X_2,...,X_n\}, \qquad V_n= \max\{X_1,X_2,...,X_n\}.$$

\begin{proof}
    We begin by observing that because $U_n$ is the minimum of all $X_i$
    \begin{align*}
        \P{U_n \geq k} & = \P{\min\{X_1  + X_2 + ... +X_n\} \geq k}                    \\
                       & =\P{X_1 \geq k \wedge X_2 \geq k \wedge ...\wedge X_n \geq k}
        \intertext{By independence of the $X_i$}
                       & = \P{X_1 \geq k}\P{X_2 \geq k}...\P{X_n \geq k}
    \end{align*}
    Now we see that $\P{X_i \geq k} = 1 - \P{X_i < k} = 1 - \frac{k-1}{N}$.
    % There are k -1 values under k out of N values.
    Hence we have that finally
    $$\P{U_n \geq k} = \left(1 - \frac{k-1}{N}\right)^n$$

    Now we also note that
    $${\P{U_n = k} = \P{U_n \geq k} - \P{U_n \geq k + 1}}$$
    Hence we can write
    \begin{align*}
        \P{U_n = k} & = \left(1 - \frac{k-1}{N}\right)^n - \left(1 - \frac{k}{N}\right)^n \\
    \end{align*}

    Now, similarly for $V_n$
    \begin{align*}
        \P{V_n \leq k} & = \P{\max\{X_1  + X_2 + ... +X_n\} \leq k}                     \\
                       & = \P{X_1 \leq k \wedge X_2 \leq k \wedge ...\wedge X_n \leq k} \\
        \intertext{By independence of the $X_i$}
                       & = \P{X_1 \leq k}\P{X_2 \leq k}...\P{X_n \leq k}                \\
    \end{align*}
    Now we can see that $\P{V_n = k} = \P{V_n \leq k} - \P{V_n \leq k-1}$
    Hence,
    $$\P{V_n = k} = \left(\frac{k}{N}\right)^n - \left(\frac{k -1}{N}\right)^n$$ \\
\end{proof}
\subsection*{3.7}
Let $X_1, X_2,...$ be discrete random variables, each having mean $\mu$ and
let $N$ be a random variable which takes values in the non-negative integers
and which is independent of the $X_i$. By conditioning on the value of $N$, show that
$$\E{X_1 + X_2 + ... + X_N} = \mu\E{N}.$$

\begin{proof}
    Let $S =X_1 + X_2 + ... + X_N$. We can see that by linearity of expectation
    $$\E{S | N =n} = \E{\sum_{i=0}^n X_i} = \mu n$$
    Hence, we have that $\E{S|N} = \mu N$. and it follows that
    $$\E{S} = \E{\E{S|N}} = \E{\mu N} = \mu \E{N}$$
\end{proof}

\subsection*{4.5} %https://math.stackexchange.com/questions/581987/probability-question-with-trees-and-fruit-using-probability-generating-functions
Each year a tree of a particular type flowers once, and the probability that it has
$n$ flowers is $(1-p)p^n, \ n = 0,1,2,..$ where $0 < p < 1$, clearly $N + 1$ is
geometrically distributed.
Each flower has probability $\frac{1}{2}$ of producing a ripe fruit, independently
of all other flowers. Find the probability that in a given year
(a) the tree produces $r$ ripe fruits.
\begin{proof}
    We can see that $N+1$ follows a geometric distribution. We also see that $X_i$, the
    event that a flower grows a fruit, follows a Bernoulli distribution. Hence we can
    find the probability generating functions
    $$G_{N+1} (s) = \frac{1}{1-ps}, \ G_{X_i} = \frac{1}{2} + \frac{1}{2}s.$$
    Hence, we can see that letting $R= X_1 + X_2 + ... + X_N$
    \begin{align*}
        G_R & = G_{N+1}(G_X(s))                             \\
            & = \frac{q}{1 - p(\frac{1}{2} + \frac{1}{2}s)}
    \end{align*}
    Now we know that for a probability generating function
    $$\P{R=r} = \frac{G^{(r)}(0)}{r!} = 2(1-p)p^r(2-p)^{-(r+1)}$$
\end{proof}
(b) the tree had $n$ flowers given that it produced $r$ ripe fruits.
\begin{proof}
    We can do this easily with Bayes' Theorem. We write
    \begin{align*}
        \P{N=n | R = r} & = \frac{\P{N=n, R=r}}{\P{R = r}}                                    \\
                        & = \frac{\P{R=r | N=n}\P{N=n}}{\P{R=r}}                              \\
                        & = \frac{\binom{n}{r}\frac{1}{2^n}p^n(1-p)}{2(1-p)p^r(2-p)^{-(r+1)}} \\
                        & = \binom{n}{r}\left(\frac{1}{2}\right)^{n+1}p^{n-r}(2-p)^{r+1}
    \end{align*}
\end{proof}
\end{document}