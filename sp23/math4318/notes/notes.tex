\documentclass[twoside]{article}

\input{./../../preamble.tex}
\geometry{letterpaper, left=20mm, right=20mm}

\begin{document}
\section{The derivative on $\R$}

\begin{definition}[ Limit point of a set ]
	Let $X \subset \R$, $x_{0} \in \R $ is a limit point of $X$ if
	\begin{equation*}
		\forall \epsilon > 0 \ \exists x \in X, x \neq n_{0} \text{ such that } | x - x_{0} | < \epsilon,
	\end{equation*}
	or
	\begin{equation*}
		\exists x_{n } \in X, x_{n } \neq x_{0}, x_{n } \to x_{0} \text{ as } n \to \infty.
	\end{equation*}
\end{definition}

\begin{definition}[ Derivative at a point ]
	Suppose $X \subset \R $ and $x_{0}$ is a limit point of $X$. then $f : X \to \R $ is differentiable at $x_{0}$ if
	\begin{equation*}
		\lim_{x \in X, x \to x_{0}} \frac{f(x) - f(x_{0})}{x - x_{0}}
	\end{equation*}
	exists. If so, we call the limit $f^{\prime}(x_{0})$.
\end{definition}

\begin{theorem}{Differentiable functions are continuous}
	If $f$ is differentiable at $x_{0}$, then $f$ is continuous at $x_{0}$.
\end{theorem}

\begin{theorem}[ Properties of derivatives ]
	Suppose $f, g : X \to \R $ are differentiable at $x_{0} \in X$. Then
	\begin{itemize}
		\item (Sums and differences) If $a, b \in \R$, then $af + bg $ is differentiable at $x_{0}$, and
		      $(af + bg )^{\prime}(x_{0}) = af^{\prime}(x_{0}) + bg^{\prime}(x_{0})$.
		\item (Product rule) The function $fg $ is differentiable at $x_{0}$ and
		      \begin{equation*}
			      (fg )^{\prime}(x_{0}) = f^{\prime }(x^{0})g(x^{0}) + f(x_{0})g^{\prime}(x_{0}).
		      \end{equation*}
		\item (Quotient rule) If $g(x^{0}) \neq 0$, then $f / g$ is differentiable at $x_{0}$, and
		      \begin{equation*}
			      (f/g)^{\prime }(x_{0}) = \frac{g(x_{0})f^{\prime}(x_{0}) - f(x_{0})g^{\prime}(x_{0})}{[g(x_{0})]^{2}}
		      \end{equation*}
	\end{itemize}
\end{theorem}

\begin{theorem}[ The chain rule ]
	Suppose $f : X \to Y$ is differentiable at $x_{0} \in X, f(x_{0})$ is a limit point of $Y \subset \R $, and
	$g : Y \to \R $ is differentiable at $f(x_{0})$. Then $g \circ f $ is differentiable, and
	\begin{equation*}
		(g \circ f )^{\prime}(x_{0}) = g^{\prime}(f(x_{0}))f^{\prime}(x_{0}).
	\end{equation*}
\end{theorem}

\begin{definition}[ Big 'O' and little 'o' ]
	For functions $f, g$, we write
	\begin{equation*}
		f(x) = O(g(x)) \text{ as } x \to a
	\end{equation*}
	if $\exists M > 0$ such that $\abs{\frac{f(x)}{g(x)}} \leq M$ for all $x$ close enough to $a $.

	Or,
	\begin{equation*}
		f(x) = o(g(x)) \text{ as } x \to a
	\end{equation*}
	if $\lim_{x \to a }\frac{f(x)}{g(x)} = 0$
\end{definition}

\begin{definition}[ Newton's Approximation ]
	$f $ if differentiable at\\
	$x_{0} \iff \abs{f(x) - [f(x_{0}) + L(x - x_{0})] = o(\abs{x - x_{0}})}$
\end{definition}

\begin{theorem}[ Fermat's theorem ]
	Suppose that $f : [a, b ] \to \R $ is differentiable on $(a,b )$ and attains its maximum at $x \in (a,b )$. Then
	$f^{\prime}(x) = 0$.
\end{theorem}

\begin{theorem}[ Rolle's Theorem ]
	Suppose that $f $ is continuous on the interval $[a, b ]$, that $f $ is differentiable on $(a,b )$ and
	$f(a ) = f(b )$. Then, there exists some $c \in (a,b )$ with $f^{\prime }(c) = 0$
\end{theorem}

\begin{theorem}[ The Mean Value Theorem ]
	Suppose that $f $ is continuous on the interval $[a, b ]$ and that $f $ is differentiable on $(a, b )$.
	Then there exists some $c \in (a, b )$ with
	\begin{equation*}
		f^{\prime}(c) = \frac{f(b) - f(a)}{b - a }
	\end{equation*}
\end{theorem}

\begin{theorem}[ Cauchy's Extended Mean Value Theorem ]
	Suppose that $f $ is continuous on the interval $[a, b ]$ and that $f $ is differentiable on $(a, b )$.
	Then there exists some $c \in (a, b )$ with
	\begin{equation*}
		f^{\prime}(c)(g(b) - g(a)) = g^{\prime}(c)(f(b) - f(a)).
	\end{equation*}
\end{theorem}

\begin{theorem}[ L'Hopital's Rule ]
	Suppose that $f$ and $g$ are both continuous functions on $[a,b]$ that are differentiable on $(a,b)$, and
	$f(a) = g(a) = 0$, but $g^{\prime}(x) \neq 0$ for all $x \in (a,b)$. Suppose
	$\lim_{x \to a^{+}} \frac{f^{\prime}(x)}{g^{\prime}(x)}$ exists. Then $g(x) \neq 0$ for all $x \in (a,b)$ and
	\begin{equation*}
		\lim_{x \to a^{+}} \frac{f(x)}{g(x)} = \lim_{x \to a^{+}} \frac{f^{\prime}(x)}{g^{\prime}(x)}
	\end{equation*}
\end{theorem}

\begin{theorem}[ Taylor's Theorem ]
	Suppose that $f$ is continuous on and interval $[a,b]$ and that $f^{\prime}, f^{\prime\prime}, \dots, f^{(k+1)}$
	exist on $[a,b]$. Then there is a $d \in [a,b]$ so at
	\begin{equation*}
		f(b) = f(a) + (b - a)f^{\prime}(a) + \dots + \frac{(b - a)^{k}}{k!}f^{(k)}(a) + \frac{(b-a)^{k+1}}{(k+1)!}f^{(k+1)}(d).
	\end{equation*}
\end{theorem}

\newpage
\section{The integral in $\R$}

\begin{definition}[ The Riemann Integral ]
	Suppose that $f : [a,b] \to \R$ is a bounded function (i.e., $\exists M < \infty \text{such that} \abs{f(x)} \leq M$
	for every $x \in [a, b]$). We would liek to define the integral
	\begin{equation*}
		\int_{a}^{b} f
	\end{equation*}
\end{definition}

\begin{definition}[ Partitions ]
	A partition $\bm{P}$ of $[a,b]$ is a collection of non-overalapping intervals whose union is $[a,b]$.

	Often written as $\bm{P} = \{a_{0}, \dots, a_{N}\}$ where $a = a_{0} < a_{1} < \dots < a_{N} = b$ to mean that\\
	$\bm{P} = \{[a_{0}, a_{1}], [a_{1}, a_{2}], \dots, [a_{N-1}, a_{N}]\}$.

	For an interval $I$ we set $\abs{I}$ to be the length of $I$.
\end{definition}

\begin{definition}[ Upper and Lower Riemann Sums ]
	Suppose $f : [a, b] \to \R$ is bounded, and $\bm{P} = {I_{1}, \dots I_{N}}$ is a partition of $[a,b]$, then
	\begin{align*}
		\mathcal{U}(f, \bm{P}) & = \sum_{j=1}^{N}(\sup_{I_{j}} f) * \abs{I_{j}} = \sum_{I \in \bm{P}} (\sup_{I} f) * \abs{I}, \\
		\mathcal{L}(f, \bm{P}) & = \sum_{j=1}^{N}(\inf_{I_{j}} f) * \abs{I_{j}} = \sum_{I \in \bm{P}} (\inf_{I} f) * \abs{I}, \\
	\end{align*}
\end{definition}

\begin{theorem}[ The fundamental inequality ]
	If $\bm{P}_{1}, \bm{P}_{2}$ are partitions of $[a,b]$, then
	\begin{equation*}
		\mathcal{U}(f, \bm{P}_{1}) \geq \mathcal{L}(f, \bm{P}_{2})
	\end{equation*}
\end{theorem}

\begin{definition}[ Refinement of a partition ]
	Suppose $\bm{P}_{1}, \bm{P}_{2}$ are partitions of $[a, b]$. We call a partition $\bm{P}_{1}$ a refinement
	of $\bm{P}_{2}$ if every interval of $\bm{P}_{1}$ is contained in an interval in $\bm{P}_{2}$.
\end{definition}

\begin{definition}[ Common refinement of two partitions ]
	Suppose $\bm{P}_{1}, \bm{P}_{2}$ are partitions of $[a, b]$, then the common refinement $\bm{P}$ of
	$\bm{P}_{1}, \bm{P}_{2}$ is defined as
	\begin{equation*}
		\bm{P} = \{I \cap J : I \in \bm{P}_{1}, J \in \bm{P}_{2} \quad \text{and} \quad I \cap J \neq \O \}.
	\end{equation*}
\end{definition}

\begin{definition}[ Riemann Integrable ]
	We call a bounded function $f : [a,b] \to \R$ \textbf{Riemann Integrable} if
	\begin{equation*}
		\sup_{\bm{P}} \mathcal{L}(f, \bm{P}) = \inf_{\bm{P}} \mathcal{U}(f, \bm{P}).
	\end{equation*}

	If $f$ is Riemann integrable, then we define
	\begin{equation*}
		\int_{a}^{b} f = \sup_{\bm{P}}\mathcal{L}(f, \bm{P}) = \inf_{\bm{P}} \mathcal{U}(f, \bm{P}).
	\end{equation*}
\end{definition}

\begin{definition}[ Another criterion ]
	We call a bounded function $f : [a,b] \to \R$ \textbf{Riemann Integrable} if and only if for every
	$\epsilon > 0$ there exists a partition $\bm{P}$ such that
	\begin{equation*}
		\mathcal{U} (f, \bm{P}) - \mathcal{L}(f, \bm{P}) < \epsilon.
	\end{equation*}
\end{definition}

\begin{theorem}[ A useful inequality ]
	If $f$ is Riemann integrable and $\abs{f(x)} \leq M$ for every $x \in [a, b]$, then
	\begin{equation*}
		-M(b-a) \leq \int_{a}^{b} f \leq M(b-a).
	\end{equation*}
\end{theorem}

\begin{theorem}[ Continuous functions are Riemann integrable ]
	Suppose $f : [a,b] \to \R$ is a continuous function, then $f$ is Riemann integrable.
\end{theorem}

\begin{definition}[ Piecewise continuous functions ]
	A bounded function $f : [a, b] \to \R$ is called \textbf{piecewise continuous} if there is a partition $\bm{P}$
	such that $f$ is continuous on $I$ for every $I \in \bm{P}$. (I.e., there are finitely many jump discontinuities)
\end{definition}

\begin{theorem}[ Piecewise functions are Riemann integrable ]
	Suppose $f : [a,b] \to \R$ is a piecewise continuous function, then $f$ is Riemann integrable.
\end{theorem}

\begin{theorem}[ Monotone functions are Riemann integrable ]
	Suppose $f : [a,b] \to \R$ is a bounded monotone function, then $f$ is Riemann integrable.
\end{theorem}

\begin{theorem}[ Algebraic properties of the Riemann integral ]
	Suppose $f, g : [a, b] \to \R$ are Riemann integrable, then:
	\begin{itemize}
		\item $f + g$ is Riemann integrable and
		      \begin{equation*}
			      \int_{a}^{b} (f + g) = \int_{a}^{b} f + \int_{a}^{b} g.
		      \end{equation*}
		\item If $\alpha \in \R$, then $\alpha f$ is Riemann integrable and
		      $\int_{a}^{b} \alpha f = \alpha \int_{a}^{b} f$.
		\item $f - g$ is Riemann integrable and
		      \begin{equation*}
			      \int_{a}^{b} (f - g) = \int_{a}^{b} f - \int_{a}^{b} g.
		      \end{equation*}
		\item $f \geq g$ for every $x \in [a, b]$, then
		      \begin{equation*}
			      \int_{a}^{b} f \geq \int_{a}^{b} g.
		      \end{equation*}
		\item $c \in (a, b)$, then $f$ is integrable on $[a, c]$ and $[c, b]$ and
		      $\int_{a}^{b} f = \int_{a}^{c} f + \int_{c}^{b} f$.
	\end{itemize}
\end{theorem}

\begin{theorem}[ Maximums and minimums ]
	If $f, g : [a, b] \to \R$ are both Riemann integrable, the so are $\max(f, g), \min(f, g)$.
\end{theorem}

\begin{corollary}
	The absolute value $\abs{f} = f^{+} - f^{-}$\\
	where $f^{+} = \max(f, 0), f^{-} = \min(f, 0)$ is Riemann integrable.
\end{corollary}

\begin{theorem}[Products of Riemann integrable]
    If $f, g : [a, b] \to \R$ are Riemann integrable, then $fg$ is Riemann integrable.
\end{theorem}

\begin{theorem}[First fundamental theorem of calculus]
    Suppose that $f : [a, b] \to \R$ is Riemann integrable. For $x \in (a,b)$ define 
    \begin{equation*}
        F(x) = \int_{a}^{x} f.
    \end{equation*}
    Then (i) $F$ is continuous on $[a,b]$, and (ii) if $f$ is continuous at $c \in [a,b]$,
    then $F^{\prime}(c) = f(c)$.
\end{theorem}

\begin{theorem}[Second fundamental theorem of calculus]
    Suppose that $f : [a,b] \to \R$ is Riemann integrable. If $F$ is an 
    anti derivative for $f$ on $[a,b]$, then 
    \begin{equation*}
        \int_{a}^{b} f = F(b) - F(a).
    \end{equation*}
\end{theorem}

\begin{theorem}[Integration by parts]
    Suppose $f, g : [a,b] \to \R$ are differentiable and $f^{\prime}, g^{\prime}$ are 
    Riemann integrable. Then 
    \begin{equation*}
        \int_{a}^{b} fg^{\prime} = f(b)g(b) - f(a)g(a) - \int_{a}^{b} f^{\prime}g.
    \end{equation*}
\end{theorem}

\begin{theorem}[Change of Variables]
	Let $g : [a, b] \to \R$ be a continuously differentiable (i.e., derivative is continuous on $[a,b]$),
	and assume that $g([a,b]) \subset [c, d]$. Let $f : [c,d] \to \R$ be continuous. Then
	\begin{equation*}
		\int_{a}^{b} f(g(x))g^{\prime}(x) dx = \int_{g(a)}^{g(b)} f(t) dt.
	\end{equation*}
\end{theorem}

\begin{theorem}[Riemann-Stieljes Integral]
	Suppose that $f : [a,b] \to \R$ is a bounded function, and $\alpha : [a,b] \to \R$ is an increasing function.
	For an interval $I = [c,d]$, we put $\alpha(I) = \alpha(d) - \alpha(c)$. If $\alpha(x) = x$, then
	$\alpha(I) = | I |$. The Riemann-Stieljes integral generalises the Riemann integral to a
	more general notion on 'length' determined by $\alpha$.

	For a partition $\bm{P}$, set
	\begin{equation*}
		\mathcal{U}(f, \bm{P}) = \sum_{I \in \bm{P}} \sup_{I} f \cdot \alpha(I) \quad \text{and} \quad
		\mathcal{L}(f, \bm{P}) = \sum_{I \in \bm{P}} \inf_{I} \cdot \alpha(I).
	\end{equation*}

	We say $f$ is $\alpha$-Riemann-Stieljes integrable if
	\begin{equation*}
		\inf_{\bm{P}}\mathcal{U}(f, \bm{P}) = \sup_{\bm{P}} \mathcal{L}(f, \bm{P}),
	\end{equation*}
	and set the common balue equal $\int_{a}^{b} f \ d \alpha$.
\end{theorem}

\begin{definition}[Uniform Convergence]
	A function $f_{n} : X \to \R$ converges uniformly to $f : X \to \R$ if
	$\forall \epsilon > 0, \exists N \in \N$ such that $|f_{n}(x) - f(x) | < \epsilon , \forall x \in X, n \geq N$
\end{definition}

\begin{theorem}[Uniform limit of continuous function is continuous]
	Let $f_{n} : [a, b] \to \R$ be continuous, if $f_{n} \to f$ uniformly on $[a,b]$.
	Then $f$ is continuous on $[a,b]$.
\end{theorem}

\begin{theorem}[Switching limits and integration]
	If $f_{n} : [a,b] \to R$ is Riemann integrable, and $f_{n} \to f$ uniformly on
	$[a,b]$. Then $f$ is Riemann integrable and
	\begin{equation*}
		\lim_{n \to \infty} \int_{a}^{b}f_{n} = \int_{a}^{b} f.
	\end{equation*}
\end{theorem}

\begin{corollary}
	If, $f_{n} : [a,b] \to \R$ is Riemann integrable, and $\sum_{n = 0}^{\infty} f_{n}$ converges uniformly,
	then $f = \sum_{n = 0}^{\infty} f_{n}$ is Riemann integrable, and
	\begin{equation*}
		\int_{a}^{b} f = \sum_{n = 0}^{\infty} \int_{a}^{b} f_{n}.
	\end{equation*}
\end{corollary}

\begin{theorem}[Interchange of limits and derivatives]
	Suppose $f_{n} : [a, b] \to \R$ are differentiable on $[a, b]$.
	Suppose that $\exists c \in [a,b]$ such that $f_{n}(c) \to f(c)$ converges as $n \to \infty$.
	Further, suppose that the derivatices $f^{\prime}_{n}$ converge uniformly on $[a, b]$
	to some function $g$. Then $f_{n}$ converges uniformly to a function $f$
	that is differentiable on $[a, b]$, and $f^{\prime} = g$.
\end{theorem}

\begin{theorem}[The Weierstrass Approximation Theorem]
	Suppose $f : [a,b] \to \R$ is a continuous function. Fix $\epsilon > 0$. There exists
	a polynomial $P$ such that $|f(x) - P(x)| < \epsilon$ for every $x \in [a,b]$.
\end{theorem}

\begin{definition}[Convolution]
	For compactly supported function $f, g : \R \to \R$, we define
	\begin{equation*}
		f * g(x) = \int_{-\infty}^{\infty} f(y)g(x - y) \ dy.
	\end{equation*}
\end{definition}

\begin{theorem}[Convolutions with polynomials are polynomials]
	Let $f : \R \to \R$ be a continuous function supported on $[0, 1]$, and
	$g : [-1, 1] \to \R$ be a continuous function that is a polynomial on
	$[-1, 1]$, then $f * g$ is a polynomial on $[0, 1]$.
\end{theorem}

\begin{definition}[$(\epsilon, \delta)$-approximation to the identity]
	We call $g : \R \to \R$ an $(\epsilon, \delta)$-approximation to identity if $g$ is
	continuous and companctly supported on $[-1, 1]$, and satisfies
	\begin{itemize}
		\item $\int_{-1}^{1} g = 1$,
		\item $g \geq 0$ on $[-1, 1]$,
		\item $|g(x)| \leq \epsilon$ for $x \in [-1, -\delta] \cup [\delta, 1]$.
	\end{itemize}
\end{definition}

\begin{corollary}[Common use of Weierstrass Approximation Thm]
    Suppose $f : [a,b] \to \R$ is a continuous function that satisfies
    \begin{equation*}
        \int_{a}^{b} f(x)x^{n} \ dx = 0
    \end{equation*}
    for all $n \in \N$, then $f(x) = 0$ for every $x \in [a,b]$.
\end{corollary}

\newpage
\section{Power and Fourier Series}
\begin{definition}[Power Series]
    A power series centered at $a \in \R$ is a formal series of the form
    \begin{equation*}
        \sum_{n = 0}^{\infty}c_{n}(x - a)^{n}
    \end{equation*}
    where $c_{n} \in \R$.
\end{definition}

\begin{definition}[Radius of convergence]
    The radius of convergence of a power series equals
    \begin{equation*}
        R = \frac{1}{\limsup_{n\to \infty}|c_{n}|^{1/n}}.
    \end{equation*}
\end{definition}

\begin{definition}
    Suppose $\sum_{n = 0}^{\infty}c_{n}(x - a)^{n}$ has radius of convergence $R > 0$.
    \begin{itemize}
        \item If $|x - a| < R$ then the series converges absolutely.
        \item If $|x - a| > R$ then the series diverges.
    \end{itemize}
    Now assume $R > 0$ and let $f: (a - R, a + R) \to \R$ defined by
    \begin{equation*}
        f(x) = \sum_{n=0}^{\infty}c_{n}(x - a)^{n}.
    \end{equation*}
    Then,
    \begin{itemize}
        \item For every $r \in (0, R)$, the series $\sum_{n = 0}^{\infty} c_{n}(x - a)^{n}$
            converges uniformly to $f$ on $[a - r, a + r]$. So $f$ is continuous.
        \item $f$ is differentiable on $(a - R, a + R)$, and for any $r \in (0, R)$,
            the series $\sum_{n = 0}^{\infty} nc_{n}(x - a)^{n - 1}$ converges
            uniformly on $(a - r, a + r)$ to the derivative $f^{\prime}$.
        \item For any closed interval $[c, d] \subset (a - R, a + R)$, $f$ is
            Riemann integrable on $[c, d]$ and 
            \begin{equation*}
                \int_{c}^{d} f = \sum_{n = 0}^{\infty}\frac{c_{n}}{n + 1} [(d - a)_{n + 1} - (c - a)^{n + 1}].
            \end{equation*}
    \end{itemize}
\end{definition}

\begin{definition}
    We call a function $f$ analytic at a point $a \in \R$ if there exists $r > 0$
    and a power series $\sum_{n = 0}^{\infty} c_{n}(x - a)^{n}$ with radius
    of convergence $\geq r$ that converges to $f$ in $(a - r, a + r)$.

    We say a function is analytic on an interval $(a, b)$ if it is analytic 
    at each point of $(a, b)$.
\end{definition}

\begin{definition}
    If $f$ is analytic on $a$ so is representted by a power series 
    $\sum_{n = 0}^{\infty}c_{n}(x - a)^{n}$ on $(a - r, a + r)$
    for some $r > 0$. Then 
    \begin{itemize}
        \item $f$ is infinitely differentiable on $(a - r, a + r)$, and
            the derivative can be found via term by term differentiation
            of the power series.
        \item If $f^{(k)}(a)$ is the $k$th derivative of $f$ at $a$, then
            \begin{equation*}
                f^{(k)}(a) = k!c_{k}.
            \end{equation*}
        \item We can therefore write $f(x) = \sum_{n = 0}^{\infty}\frac{f^{(n)}(a)}{n!}(x - a)^{n}$.
    \end{itemize}
\end{definition}

\begin{theorem}
    Suppose that for some $a \in \R$ and $r > 0, \sum_{n = 0}^{\infty}c_{n}(x - a)^{n}$
    and $\sum_{n = 0}^{\infty}d_{n}(x - a)^{n}$ are two power series
    with radius of convergence $\geq r$ and 
    \begin{equation*}
        \sum_{n = 0}^{\infty}a_{n}(x - a)^{n} = \sum_{n= 0}^{\infty}b_{n}(x - a)^{n}
        \text{ for every } x \in (a - r, a+ r).
    \end{equation*}
    Then $c_{n} = d_{n}$ for all $n$.
\end{theorem}


\begin{definition}[Double Series]
    Consider the numbers $\{a_{i, j} : i,j \in \N\}$. Define the partial sum 
    \begin{equation*}
        S_{m,n} = \sum_{1 \leq i \leq m, 1 \leq j \leq n} a_{i,j}.
    \end{equation*}

    We say $\sum_{i,j} a_{i,j}$ converges to $x$ if there exists $N \in \N$ such that
    $|S_{m,n} - x| < \epsilon$ when $m,n \geq N$.

    We say $\sum_{i,j} a_{i,j}$ converges absolutely if the series $\sum_{i,j} |a_{i,j}|$
    converges, which means the set 
    \begin{equation*}
        \left\{\sum_{i = 1}^{m}\sum_{j = 1}^{n} |a_{i,j}| : n,m \in \N \right\}
    \end{equation*}
    is bounded by a set of real numbers.
\end{definition}

\begin{theorem}[Convergence of Double Series]
    If $\sum_{i,j} a_{i,j}$ converges absolutely then $\sum_{i,j} a_{i,j}$ converges.
\end{theorem}

\begin{theorem}
    If $\sum_{i,j} |a_{i,h}|$ converges, then 
    \begin{equation*}
        \sum_{i,j} a_{i,j} = \sum_{i}\sum_{j} a_{i,j}
        = \sum_{j}\sum_{i} a_{i, j} = \sum_{k}\sum_{i+j=k} a_{i,j}.
    \end{equation*}
    The last form is called \textit{Cauchy product}.
\end{theorem}

\begin{definition}[Multiplication of Absolutely Convergent Series]
    Suppose $\sum_{n = 0}^{\infty}x_{n}$ and $\sum_{n = 0}^{\infty}y_{n}$ are
    absolutely convergent series, then 
    \begin{equation*}
        \sum_{n=0}^{\infty}\sum_{m=0}^{\infty} = \sum_{n=0}^{\infty}z_{n}
    \end{equation*}
    where
    \begin{equation*}
        z_{n} = \sum_{i+j = n} x_{i}y_{i}.
    \end{equation*}
\end{definition}

\begin{theorem}[Product of Absolutely Convergent Power Series]
    Suppose that $f(x) = \sum_{n = 0}^{\infty}c_{n}(x - a)^{n}$ and $g(x) = \sum_{n=0}^{\infty}d_{n}(x-a)^{n}$
    are analytic on the interval $(a - R, a + R)$. Then, $f(x)g(x)$ is analytic
    on $(a - R, a + R)$ and 
    \begin{equation*}
        f(x)g(x) = \sum_{n = 0}^{\infty}e_{n}(x-a)^{n},
    \end{equation*}
    where $e_{n} = \sum_{i + j = n}c_{i}d_{i}$.
\end{theorem}

\begin{definition}[Exponential Function]
    We define 
    \begin{equation*}
        \exp(x) = \sum_{n = 0}^{\infty}\frac{x^{n}}{n!}.
    \end{equation*}
\end{definition}

\begin{theorem}[Inverse Function Theorem]
    Suppose that $f : (a,b) \to (c,d)$ is a continuous invertible function 
    and $f^{-1} : (c,d) \to (a,b)$ is continuous. If $f$ is differentiable
    at $x_{0} \in (a,b)$ and $f^{\prime}(x_{0}) \neq 0$, then 
    $f^{-1}$ is differentiable at $y_{0} = f(x_{0})$ and,
    \begin{equation*}
        f^{-1}(y_{0})^{\prime} = \frac{1}{f^{\prime}(x_{0})}.
    \end{equation*}
\end{theorem}

\begin{theorem}
    Suppose $f : (a,b) \to (c,d)$ is stricly increasing, continuous and invertible,
    then $f^{-1}: (c,d) \to (a,b)$ is stricly increasing and continuous.
\end{theorem}

\begin{definition}
    Since $\exp(x) : \R \to (0, \infty)$ is stricly increasing continuous and differentiable,
    it has an inverse function $\log : (0, \infty) \to \R$ such that
    \begin{enumerate}
        \item $\log^{\prime}(x) = \frac{1}{x}$ for $x \in (0, \infty)$
        \item $\log$ satisfied laws of logarithms
        \item For $x \in (-1, 1)$, $\log x$ has power series expansion
            \begin{equation*}
                \log(1 - x) = -\sum_{n = 1}^{\infty} \frac{x^{n}}{n}.
            \end{equation*}
    \end{enumerate}
\end{definition}

\begin{definition}[Metric]
    Let $X$ be a set. We call a function $d: X \times X \to [0, \infty)$ a \textit{metric}
    if for any $x, y, z \in X$ we have:
    \begin{itemize}
        \item $d(x,y) = 0 \iff x = y$,
        \item $d(x,y) = d(y,x)$,
        \item $d(x,z) \leq d(x,y) + d(y, z)$.
    \end{itemize}
\end{definition}

\begin{theorem}[Convergence in metric spaces]
    Let $(X, d)$ be a metric space. We say a sequence $\{x_{n}\}n$ with $x_{n} \in X$,
    converges to $x \in X$ if 
    \begin{equation*}
        \lim_{n \to \infty}d(x_{n}, x) = 0,
    \end{equation*}
    i.e., for every $\epsilon > 0, \exists N>0$ such that $d(x_{n}, x) < \epsilon$
    for all $n \geq N$.
\end{theorem}

\begin{definition}[Cauchy sequence]
    Let $(X, d)$ be a metric space, then $\{x_{n}\}_{n}$ is a Cauchy sequence
    in $X$ if for every $\epsilon > 0$ there exists $N \in \N$ such that
    $d(x_{n}, x_{m}) < \epsilon$ for every $n, m \geq N$.
\end{definition}

\begin{definition}[Complete metric space]
    $(X, d)$ is called a \textit{complete metric space} if every Cauchy sequence to a
    limit in $X$.
\end{definition}

\begin{definition}[Compact metric space]
    $(X, d)$ is compact if every sequence in $X$ has a convergent subsequence.

    Recall Heine-Borel Thm. A subset of $\R$ is compact if and only if it it 
    is closed and bounded.
\end{definition}

\begin{definition}[Closed metric space]
    Let $(X, d)$ be a metric space and $E \subset X$. Then, $E$ is \textit{closed}
    if whenever $x_{n}$ is a sequence in $E$, and $x_{n} \to x$ 
    and $n \to \infty$, then $x \in E$.
\end{definition}

\begin{theorem}[Compact sets are closed and bounded]
    Suppose $(X, d)$ is a compact metric space, then $X$ is closed and bounded.
    
    Warning: the converse is not true.
\end{theorem}

\begin{definition}[Open ball]
    Let $(X, d)$ be a metric space. We define the open ball as 
    \begin{equation*}
        B(x_{0}, r) = \{x \in X : d(x, x_{0}) < r\}.
    \end{equation*}
\end{definition}

\begin{definition}[Open set]
    Let $(X, d)$ be a metric space. The set $U \subset X$ is \textit{open} if 
    $\forall x \in U, \exists r > 0, r = r(x)$ such that 
    $B(x, r) \subset U$.
\end{definition}

\begin{definition}[Totally bounded set]
    Let $(X, d)$ be a metric space and $E \subset X$. Then, $E$ is 
    called \textit{totally bounded} if for every $\epsilon > 0$,
    there exists $N \in \N$ and $x_{1}, \dots, x_{N} \in X$
    such that
    \begin{equation*}
        E \subset \bigcup_{j = 1}^{N} B(x_{j}, \epsilon).
    \end{equation*}
\end{definition}

\begin{theorem}[Heine-Borel in metric spaces]
    Let $(X, d)$ be a metric space and $E \subset X$. 
    Then $E$ is compact if and only if $E$ is closed and totally bounded.
\end{theorem}

\begin{definition}[Compactness is terms of open balls]
    Let $(X, d)$ be a metric space and $E \subset X$. Then, $E$ is compact
    if and only if for every collection of open sets $\{U_{\alpha}\}_{a}$
    such that $X \subset \bigcup_{\alpha} U_{a}$, there is a finite 
    subset $U_{a_{1}}, \dots, U_{\alpha_{N}}$ such that 
    $X \subset U_{j = 1}^{N} U_{\alpha_{j}}$.
\end{definition}

\begin{theorem}[Complex numbers are complete]
    $(\C, |\cdot|)$ is a complete metrix space.
\end{theorem}

\begin{definition}[Complex trig fns]
    For $z \in \C$ we define 
    \begin{equation*}
        \cos z = \frac{1}{2}(e^{iz} + e^{-iz}) \text{ and }
        \sin z = \frac{1}{2i}(e^{iz} - e^{-iz}).
    \end{equation*}
\end{definition}

\begin{definition}[1-periodic functions]
    We call a function $f: \R \to \C$ 1-periodic if $f(x + 1) = f(x)$.
    We define $\mathbb{T} = \R \setminus \Z$ (the interval $[0, 1]$
    with 0 and 1 identified with each other). Then, $C(\mathbb{T}, \C)$
    denoted the collection of all continuous 1-periodic functions
    from $\R$ to $\C$.

    A function $f \in C(\mathbb{T}, \C)$, we have $f(t) \in \C, \forall t$,
    so $f(t) = u(t) + iv(t)$ where $u, v$ are continuous real valued
    1-periodic functions.
\end{definition}

\begin{theorem}[Properties of 1-periodic functions]
    Let $f \in C(\mathbb{T}, \C)$, then,
    \begin{itemize}
        \item $f$ is bounded, i.e., $\exists M > 0$ such that $|f(t)| \leq M, \forall t \in [0, 1]$.
        \item $C(\mathbb{T}, \C)$ is a vector space.
        \item If $f_{n} \in C(\mathbb{T}, \C)$ converges uniformly to a function $f$, i.e.,
            \begin{equation*}
                d_{\infty}(f^{n}, f) = \sup_{t \in [0, 1]} |f_{n}(t) - f(t)| \to 0 \text{ as } n \to \infty,
            \end{equation*}
            then $f \in C(\mathbb{T}, \C)$. This means that the metric space 
            $(C(\mathbb{T}, \C), d_{\infty})$ is a \textit{complete metric space}.
    \end{itemize}
\end{theorem}

\begin{definition}[Integral on $C(\mathbb{T}, \C)$]
    For $f \in C(\mathbb{T}, \C)$ we have $f = u + iv$ where $u, v$ are continuous 
    real valued functions. So $u, v$ are Riemann integrable. We define 
    \begin{equation*}
        \int_{0}^{1} f(t) \ dt = \int_{0}^{1} u(t) \ dt + i \int_{0}^{1}v(t) \ dt.
    \end{equation*}
\end{definition}

\begin{definition}[$L^2$-Inner product]
    For $f, g \in C(\mathbb{T}, \C)$ let 
    \begin{equation*}
        \langle, f, g \rangle_{2} = \int_{0}^{1} f(t) \overline{g(t)} \ dt.
    \end{equation*}
\end{definition}

\begin{theorem}[Properties of $L^2$-inner product]
    For $f, g, h \in C(\mathbb{T}, \C), \alpha \in \C$,
    \begin{itemize}
        \item $\langle g, f \rangle_{2} = \overline{\langle f, g \rangle}_{2}$.
        \item $\langle f, f, \rangle_{2} = 0$ if and only if $f(t) = 0 \ \forall t$.
        \item $\langle f + g, h \rangle_{2} = \langle f, h \rangle_{2} + \langle g, h \rangle_{2}$
            and $\langle \alpha f, g \rangle_{2} = \alpha \langle f, g \rangle$.
        \item $\langle f, g + h \rangle_{2} = \langle f, g \rangle_{2} + \langle f, h \rangle_{2}$
            and $\langle f, \alpha g \rangle_{2} = \overline{\alpha} \langle f, g \rangle$.
    \end{itemize}
\end{theorem}

\begin{definition}[$L^2$-norm and metric]
    We define 
    \begin{equation*}
        \lVert f \rVert_{2} = \sqrt{\langle f, f \rangle_{2}} = \left(\int_{0}^{1} |f(t)|^2 \ dt\right)^{1/2}.
    \end{equation*}

    For $f, g \in L^{2}$ we set 
    \begin{equation*}
        d_{L^{2}}(f,g) = \lVert f - g \rVert_{2} = \left(\int_{0}^{1} |f(t) - g(t)|^2 \ dt\right)^{1/2}.
    \end{equation*}
\end{definition}

\begin{theorem}[Cauchy-Schwarz inequality]
    For $f, g \in C(\mathbb{T}, \C)$,
    \begin{equation*}
        |\langle f, g \rangle_{2}| \leq \lVert f \rVert_{2} \lVert g \rVert_{2}.
    \end{equation*}
\end{theorem}

\begin{definition}[Trigonometric polynomials]
    For $n \in \Z$ we set $e_{n}(t) = e^{2 \pi i n t}$.

    These $e_{n}(t)$ form an orthonormal sequence:
    \begin{itemize}
        \item $\langle e_{n}, e_{m} \rangle = 0$ if $n \neq m$, and 
        \item $\norm{e_{n}}_{2} = 1$ for all $n$.
    \end{itemize}
\end{definition}

\begin{definition}
    A trigonometric polynomial is a function of the form 
    \begin{equation*}
        f(t) = \sum_{n = -N}^{N} c_{n}e_{n}(t), \text{ where } c_{n} \in \C.
    \end{equation*}

    Important property:
    Suppose $f$ is a trig. polynomial. Then, $c_{n} = \langle f, e_{n} \rangle$, and 
    \begin{equation*}
        \norm{f}_{2}^{2} = \sum_{n = -N}^{N} \abs{c_{n}}^{2}.
    \end{equation*}
\end{definition}

\begin{theorem}[Pythagoras\' Theorem]
    If $\langle f, g \rangle = 0$ then 
    \begin{equation*}
        \norm{f + g}_{2}^{2} = \norm{f}_{2}^{2} + \norm{g}_{2}^{2}.
    \end{equation*}
\end{theorem}

\begin{definition}[Fourier Coefficient]
    Suppose $f \in C(\mathbb{T}, \C)$. For $n \in \Z$ we define
    \begin{equation*}
        \hat{f}(n) = \langle f, e_{n} \rangle 
        = \int_{0}^{1} f(t)\overline{e_{n}(t)} \ dt 
        = \int_{0}^{1} f(t) e^{-2 \pi int} \ dt.
    \end{equation*}

    Therefore, if $f$ is a trigonometric polynomial (i.e., $f = \sum_{n = -N}^{N} c_{n}e_{n}$)
    then 
    \begin{equation*}
    f(t) = \sum_{n = -N}^{N} \hat{f}(n) e_{n}(t) \quad \text{and} \quad
    \norm{f}_{2}^{2} = \sum_{-N}^{N} \abs{\hat{f}(n)}^{2}.
    \end{equation*}
\end{definition}

\begin{theorem}[Periodic Weierstrass Approximation Thm.]
    Suppose $f \in C(\mathbb{T}, \C)$, then there is a sequence 
    $P_{n} = P_{n}(f)$ of trigonometrix polynomials that converge 
    uniformly to $f$, i.e., 
    \begin{equation*}
        \norm{f - P_{n}}_{\infty} \to 0 \text{ as } n \to \infty.
    \end{equation*}
    which is equivalent to 
    \begin{equation*}
        \sup_{t \in \R} \norm{f(t) - P_{n}(t)} \to 0 \text{ as } n \to \infty.
    \end{equation*}
\end{theorem}

\begin{theorem}[Periodic $L_2$ convergence]
    Suppose $f \in C(\mathbb{T}, \C)$, then with 
    $P_{N} = \sum_{n = -N}^{N} \hat{f(n)}e_{n}$ we have that 
    \begin{equation*}
        \norm{f - P_{N}}_{2} \to 0 \text{ as } N \to \infty.
    \end{equation*}
\end{theorem}

\begin{theorem}
    If $f \in C(\mathbb{T}, \C)$ then 
    \begin{equation*}
        \norm{f}_{2}^{2} = \sum_{-\infty}^{\infty}\abs{\hat{f}(n)}^{2}.
    \end{equation*}
\end{theorem}




\end{document}
