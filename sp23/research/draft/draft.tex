\documentclass[twoside]{article}

\input{./../../preamble.tex}

\begin{document}
\begin{definition}
	We define a shallow neural network with $N$ neurons to be a function $\mathcal{N} : \R^{d} \to \R$
	of the form
	\begin{equation*}
		\mathcal{N}_{\omega, c} = \sum_{n=1}^{N}c^{n} \sigma(\bm{w}_{n} \cdot [\bm{x}, 1]^{T}),
	\end{equation*}
	where $\sigma(\bm{w}_{n} \cdot [\bm{x}, 1]^{T})$ are the single neurons, $N$ is the width of the network,
	and $\bm{\omega} = (a_{n}, b_{n}) \in \R^{d + 1}$ denoted the nodes with inner
	weights $a_{n} \in \R^{n}$ and $b_{n} \in \R$. The numbers $c_{n} \in \R$ are called
	the outer weights.
\end{definition}

\begin{definition}
	We let $L(\cdot)$ denote the least squares loss function
	\begin{equation*}
		L(\mathcal{N}_{\omega, c} ; \bm{y}) = L_{K, x}(\mathcal{N}_{\omega, c} ; \bm{y})
		:= \frac{1}{2K}\sum_{k = 1}^{K} \lvert \mathcal{N}_{\omega, c}(x_{k}) - y_{k} \rvert^{2}.
	\end{equation*}
\end{definition}

\begin{lemma}\label{lemma:equality-of-problems}
	Let
	\begin{align*}
		I(w, c) & = L(\mathcal{N}_{\omega, c} ; \bm{y}) + \alpha \sum_{n = 1}^{N} \phi(\rvert c_{n} \lvert), \\
		J(w, c) & =  L(\mathcal{N}_{\omega, c} ; \bm{y})
		+ \alpha \sum_{n=1}^{N} \phi\left(\frac{\abs{c_{n}}^{2} + \lVert w_{n} \rVert^{2}}{2}\right).
	\end{align*}

	For any $(\hat{\omega}, \hat{c}) \in (\mathbb{S}^{d} \times \R)^{N}$, there exists a
	$(\bar{\omega}, \bar{c}) \in (\R^{d + 1} \times \R)^{N}$
	such that $I(\hat{\omega}, \hat{c}) = J(\bar{\omega}, \bar{c})$.
\end{lemma}

\begin{proof}
	Let $(\hat{\omega}_n, \hat{c}_n) \in \mathbb{S}_{d} \times \R^{N}$.
	Now, let $\bar{\omega}_n = \frac{\hat{\omega}_n}{\sqrt{|\hat{c}_n |}}$ and
	$\bar{c}_n = \hat{c}_n \cdot \sqrt{|\hat{c}_n|}$.

	Now consider
	\begin{align*}
		I(\hat{\omega}_n, \hat{c}_n) & = \lVert \mathcal{N}_{\hat{\omega}, \hat{c}} - \bm{y} \rVert_{2}
		+ \alpha \sum_{n = 1}^{N} \phi(|\hat{c}_n|).                                                    \\
	\end{align*}
	Note that due to the positive homogeneity of ReLU we then have that
	$\lVert \mathcal{N}_{\hat{\omega}, \hat{c}} - \bm{y} \rVert_{2} = \lVert \mathcal{N}_{\bar{\omega}, \bar{c}} - \bm{y} \rVert_{2} $,
	and also that $|\hat{c}_n | = \frac{|\bar{c}_n|^2 + \lVert \bar{\omega}_n \rVert^2}{2}$.
	Therefore, $I(\hat{\omega}, \hat{c}) = J(\bar{\omega}, \bar{c})$.

\end{proof}

\begin{theorem}
	Let $L$ be a loss function and $D = (\bm{x}_{n}, \bm{y}_{n})_{n=1}^{d}$ be training examples. Then
	\begin{equation}
		\min_{N \in \N, \{c_{n}\} \in \R^{N}\, \{\omega_{n} = (a_{n}, b_{n})\} \in (\mathbb{S}^{d})^{N}}
		I(w, c),
	\end{equation}
	where $\mathbb{S}^{d} = \{(a,b) \in \R^{d + 1} : \lVert {(a,b)}^{2} \rVert_{2} = 1\}$ is the unit sphere in $\R^{d + 1}$.

	Is equivalent to
	\begin{equation}
		\min_{N \in \N, \{(a_{n}, b_{n}, c_{n})\}\in (\R^{d} \times \R \times \R)^{N}}
		J(w, c),
	\end{equation}
	By equivalence we mean that
	if $(\hat{w}, \hat{c})$ is a solution to (1), then, there exists
	$(\bar{w}, \bar{c})$ that is also a solution to (2), and viceversa.
\end{theorem}

\begin{proof}
	Let $g: (\mathbb{S}^{d} \times \R) \to (\R^{d + 1}\times \R)$ and
	$h: (\R^{d + 1}\times \R) \to (\mathbb{S}^{d} \times \R)$.

	We note that due to the increasing nature of $\phi$,
	and the inequality between arithmetic and geometric sums,
	\begin{equation}\label{eq:problem-inequality}
		I(\bm{w}, \bm{c}) \leq J(\bm{w}, \bm{c})
		\quad \forall (\bm{w}, \bm{c}) \in (\R^{d + 1}\times \R).
	\end{equation}

    NOTE: problem part

	$(1) \Rightarrow (2)$.
	Let $(\hat{w}^{*}, \hat{c}^{*})$ minimise $I$. By Eq. (\ref{eq:problem-inequality}) and
	Lemma \ref{lemma:equality-of-problems} we get
	\begin{equation*}
		I(\hat{w}^{*}, \hat{c}^{*}) \leq J(\hat{w}^{*}, \hat{c}^{*})
		\stackrel{?}{=} I(h(\hat{w}, \hat{c})) \leq I(\hat{w}^{*}, \hat{c}^{*}).
	\end{equation*}
	Therefore, $I(\hat{w}^{*}, \hat{c}^{*}) = J(g(\hat{w}^{*}, \hat{c}^{*}))$.

    NOTE: end of problem part


	$(2) \Rightarrow (1)$
	Let $(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*})$ minimise $J$.
	Now, by Eq. (\ref{eq:problem-inequality}) and Lemma \ref{lemma:equality-of-problems} we have that
	\begin{equation}\label{eq:J-equals-I}
		J(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*}) \geq I(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*})
		= J(g(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*})) \geq J(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*}).
	\end{equation}
	Therefore, $J(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*}) = I(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*})$.

	\begin{claim}
		$(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*})$ minimises $I$.

		\begin{proof}
			Assume, for the sake of contradiction, that it does not. this implies
			$\exists \hat{\bm{w}} \in (\mathbb{S}^{d} \times \R)$ such that
			\begin{equation*}
				I(\hat{\bm{w}}, \hat{\bm{c}}) < I(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*}).
			\end{equation*}
			However, from Lemma \ref{lemma:equality-of-problems} and Eq. (\ref{eq:J-equals-I})
			we get that this implies
			\begin{equation*}
				J(g(\hat{\bm{w}}^{*}, \hat{\bm{c}}^{*})) < J(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*}),
			\end{equation*}
			which is a contradiction as $(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*})$ minimises $J$.
		\end{proof}
	\end{claim}
	Therefore, we have that if $(\bar{\bm{w}}^{*}, \bar{\bm{c}}^{*})$ minimises $J$, then,
    it is also minimises $I$.
\end{proof}

\end{document}
