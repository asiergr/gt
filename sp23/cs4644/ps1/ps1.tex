\documentclass{article} 
\input{./../../preamble.tex}

\begin{document}
\section{Collaborators}
I have not collaborated with anyone on this homework.

\section{Optimization}
\subsection{}
\begin{proof}
    We will show that $\grad f $ is orthogonal to $\pdv{\vec{r}}{t}\big|_{t_0}$. To do this, we will show that $\grad f_{0} \cdot \pdv{\vec{r}}{t}\big|_{t_{0}} = 0$.
    We have that $x_{0} \in \R$ and that $L_{f(x_{0})}$ is a level surface passing through this point. The gradient at this point is 
    $\grad f_{0}$

    Now, we also have that $\vec{r} = [x_{1}(t), \dots, x_{d}(t)]$ is a curve passing through $x_{0}$. Therefore, $\exists t_{0} \in \R$
    such that $\vec{r}(t_{0}) = x_{0}$. We also know that $\vec{r}(t) \in L_{f(x_{0})} \text{ for all } t$.

    We start by noting that
    \begin{align*}
        \grad f_{0} \cdot \pdv{\vec{r}}{t} &= \sum_{k=1}^{d}\pdv{f}{x_{k}}\big |_{x_{0}}\pdv{x_{k}}{t}\big |_{t_{0}},\\
                                           \intertext{now, by the chain rule}
                                           &= \sum_{k=1}^{d} \pdv{f}{t} \big |_{x_{0}}
    \end{align*}
    Now, we know that since $\vec{r}$ lies on the level surface, then we always have that $f(\vec{r}) = c$. Taking the derivative
    \begin{equation*}
        \dv{f}{r}\dv{r}{t} = \dv{f}{t} = \dv{}{t}c = 0
    \end{equation*}

    Therefore, we finally get that $\grad f_{0} \cdot \pdv{\vec{r}}{t} = \sum_{k=1}^{d} \pdv{f}{t} \big |_{x_{0}} = 0$.
\end{proof}

We have just proven that the directon of steepest descent of any level curve is orthogonal to the tangent of any arbitrary curve living
in this level surface at any given point. This is useful to know because we can now find the gradient using the cross product of the
tangent line of this arbitrary curve and another vector that is orthogonal to the gradient (something like the standard vector in the 
$d+1$th dimension could work). This might be easier to compute than the gradient itself.

\newpage
\subsection{}
Consider a (not necessarily convex) differentible function $g: \R^{n} \to \R$. If $g$ has a local minimum at some $\mathbf{w}_{t}$ 
if there exists some $\gamma > 0$ such that for all 
$\mathbf{w} \in \R^{n}, \parallel \mathbf{w}^{t} - \mathbf{w} \parallel_{2} < \gamma \implies g(w^{t}) \leq g(\mathbf{w})$

Prove that if $g$ has a local minim at some $\mathbf{w^{t}}$ then the gradient at $\mathbf{w}^{t} = 0$ and that the converse is not 
necessarily true.

\begin{proof}
    We will prove this by contraction. That is, assume that $\grad g(\mathbf{w}^{t}) \neq 0$.
    Let $\gamma > 0$ and $\mathbf{w} \in R^{n}$ such that 
    $\pnorm{2}{\mathbf{w}^{t} - \mathbf{w}} < \gamma$.  Without loss of generality, we assume that 
    $\grad g(\mathbf{w}^{t}) = [c, 0, \dots, 0]$ where $c \in \Z_{> 0}$. This implies that there exists some $\epsilon > 0$ such that
    with $\mathbf{\epsilon} = [\epsilon, 0, \dots, 0]$ we have that $\pnorm{2}{\mathbf{w}^{t} - (\mathbf{w} + \mathbf{\epsilon})} < \gamma$
    but $g(\mathbf{w} + \mathbf{\epsilon}) < g(\mathbf{w}^{t})$, which is a contradiction.

    The reason we can say this without loss of generality is because if the gradient is nonzero in other dimesnions, then the
    $\mathbf{\epsilon}$ vector will simply also have other dimensions. And if $c$ is negative, then we subtract $\mathbf{\epsilon}$
    instead of adding it.

    To prove that the converse is not necessarily true, we can consider a saddle point. We know that on a saddle point the gradient
    is zero, but it is clearly not a minimum (by definition).
\end{proof}

\newpage
\subsection{}
Prove that if a differentiable function $g: \R^{n} \to \R$ is convex and the gradient at some $\mathbf{w}^{*}$ is 0, then 
$\mathbf{w}^{*}$ is the global minimum of $g$.

\begin{proof}
    We claim that any function $f: \R^{n} \to \R$ is convex if $f(\vec{y}) > f(\vec{x}) + \grad f(\vec{x})^{T}(\vec{y} - \vec{x})$.
    for all $\vec{x}, \vec{y} \in \R^{n}$
    \begin{proof}
        We begin by noting that
        \begin{equation*}
            \grad f(\vec{x})^{T}(\vec{y} - \vec{x}) = \sum_{n} \pdv{f}{x_{k}}(y_{k} - x_{k})\\
        \end{equation*}
        Now, looking at each summation term
        \begin{equation*}
            \pdv{f}{x_{k}}(y_{k} - x_{k}) = (y_{k} - x_{k})\lim_{h \to 0} \frac{f(x_{k} + h) - f(x)}{h}
        \end{equation*}
        Then, we let $h = t(y_{k} - x_{k})$ and get that
        \begin{align*}
            \pdv{f}{x_{k}}(y_{k} - x_{k}) &= \lim_{t \to 0} \frac{f(x_{k} + t(y_{k} - x_{k})) - f(x)}{t},
            \intertext{since $g$ is convex, we then have that}
                                          &\leq \lim_{t \to 0} \frac{(1 - t)f(x_{k}) + tf(y_{k}) - f(x_{k})}{t}\\
                                          &= f(y_{k}) - f(x_{k})
        \end{align*}
        Therefore, we have that $f(\vec{y}) > f(\vec{x}) + \grad f(\vec{x})^{T}(\vec{y} - \vec{x})$ as required.
    \end{proof}

    Now that we have proven this intermediate result, it becomes evident that if $\grad f(\mathbf{w}^{*}) = 0$ then we have that 
    $f(\mathbf{w}) > g(\mathbf{w}^{*}) \text{ for all } \mathbf{w} \in \R^{n}$. Which concludes our proof.
\end{proof}

\newpage
\subsection{Softmax}
\begin{proof}
    We are to find the jacobian $\mathbf{J}$ of the softmax function $\mathbf{s}(\mathbf{z})$. That is, we are to find
    \begin{equation*}
        \pdv{s_{i}}{z_{k}} \text{ for all } i, j \in [0, k]
    \end{equation*}

    We find that there are two cases, one when $i = j$ and another when $i \neq j$.

    Case I: $i = j$

    \begin{align*}
        \pdv{s_{i}}{z_{i}} &= \pdv{}{z_{i}} \frac{e^{z_{i}}}{\sum_{k}e^{z_{k}}},\\
                           &= \frac{e^{z_{i}}\sum_{k}e^{z_{k}} - e^{z_{i}}e^{z_{i}}}{\left[\sum_{k}e^{z_{k}}\right]^{2}},\\
                           &= \frac{e^{z_{i}\left[\sum_{k}e^{z_{k}} - e^{z_{i}}\right]}}{\left[\sum_{k}e^{z_{k}}\right]^{2}},\\
                           &= s_{i}\left[\frac{\sum_{k}e^{z_{k}} - e^{z_{i}}}{\sum_{k}e^{z_{k}}}\right],\\
                           &= s_{i}(1 - s_{i}).
    \end{align*}

    Case II: $i \neq j$
    \begin{align*}
        \pdv{s_{i}}{z_{i}} &= \pdv{}{z_{i}} \frac{e^{z_{i}}}{\sum_{k}e^{z_{k}}},\\
                           &= \frac{e^{z_{i}}e^{z_{j}}}{\left[\sum_{k}e^{z_{k}}\right]^{2}},\\
                           &= -s_{i}s_{j}
    \end{align*}

    Therefore we have that
    \begin{equation*}
        \pdv{s_{i}}{z_{j}} = \begin{cases}
            s_{i}(1 - s_{i}) & i = j,\\
            -s_{i}s_{j} & i \neq j,\\
            \end{cases}
    \end{equation*}
\end{proof}

\newpage
\subsection{Extra credit}
\begin{proof}
    The KKT conditions allow us to solve a problem of the form
    \begin{equation*}
        (P) = \begin{cases}
            \text{optimise } f(\mathbf{y}),\\
            \text{such that } g(\mathbf{y}) = 0\\
            \end{cases}
    \end{equation*}

    Thus, we are to optimise the function
    \begin{equation*}
        \argmin_{y \in \R^{d}} - \mathbf{x}^{T}\mathbf{y} - H(\mathbf{y}),
        \text{ such that } \mathbf{1}^{T}\mathbf{y} = 1, 0 \leq y_{i} \leq 1 \text{ for all } i.
    \end{equation*}

    For this problem, we have the Lagrangian function
    \begin{equation*}
        \mathcal{L} (\vec{y}, \alpha) = f(\vec{y}) + \alpha^{T}\mathbf{g}(\mathbf{y}).
    \end{equation*}

    Since $\R^{d}$ is a convex set, we can find the saddle point of this Lagrangian to find the optimal $(x^{*}, \alpha^{*})$
    for this Lagrangian. This solution, by the KKT Theorem, is then the optimal solution to the primary problem $(P)$.

    We begin by expressing the Lagrangian in terms of the functions given in the problem.
    \begin{equation*}
        \mathcal{L}(\mathbf{y}, \alpha) = -\mathbf{x}^{T}\mathbf{y} - H(\mathbf{y}) + \alpha(\mathbf{1}^{T}\mathbf{y} - 1).
    \end{equation*}
    
    Now, calculating $\grad_{\mathbf{y}} \mathcal{L}(\mathbf{y, \alpha}) = 0$ yields, for each $i$
    \begin{gather*}
        -x_{i} + \log_{}y_{i}^{*} + 1 + \alpha^{*} = 0,\\
        \log_{}y_{i}^{*} = x_{i} - 0 - \alpha^{*},\\
        y_{i} = \exp{x_{i} - 1 - \alpha^{*}}.
    \end{gather*}

    We now turn our attention to the constraint $\mathbf{1}^{T}\mathbf{y} = 1$. This is simply the sum along the components of
    $\mathbf{y}$ and can be expressed as
    \begin{align*}
        \mathbf{1}^{T}\mathbf{y} &= \sum_{k}y_{i},
        \intertext{substituting in the value for $y_{i}$ we just found we get}
                                 &= \sum_{k}\exp{x_{n} - 1 - \alpha^{*}},\\
                                 &= \exp{-\alpha^{*}}\sum_{k}\exp{x_{k} - 1} = 1.\\
        \intertext{Taking the natural log of each side and rearranging we get}
                      \alpha^{*} &= \ln \sum_{k}\exp{x_{k} - 1}
    \end{align*}

    Finally, we can substitute this value into the equation for $y_{i}$, which results in
    \begin{align*}
        y_{i}^{*} &= \exp{x_{i} - 1 - \ln \sum_{k}\exp{x_{k} - 1}},\\
                  &= \frac{\exp{x_{i} - 1 + 1}}{\exp{\ln \sum_{k}x_{k} - 1}},\\
                  &= \frac{e^{x_{i}}}{\sum_{k}e^{x_{k}}},\\
                  &= s_{i}(\mathbf{x})
    \end{align*}

    Which concludes our proof.

    The interesting thing about this representation happens if we take each vertex of this $d - 1$ simplex as a "class". Then, the 
    softmax layer will project our point onto this simplex. The position in the projection is nothing but a way of measuring
    how far the projected vertex is to each ("class"). That is, the softmax layer is simply providing us with a way to tell
    what class the point is closest to in this probabilistic fashion, considering a simplex.
\end{proof}

\newpage
\section{Dag and Topological sorting}
\begin{proof}
    We will prove that if $G$ is a DAG, then $G$ has a topological ordering.

    We will do a proof by inducting on $\sz{V} = k$.
    \textbf{Base Case:} We take a DAG $G_{0}$ of size $\sz{V} = 1$. Clearly this has a topological ordering: the only vertex $v \in V$.

    \textbf{Inductive assumption:} We assume that a dag $G_{1}$ such that $\sz{V} = k$ has a topological ordering.

    \textbf{Inductive step:} We now consider a dag $G_{2}$ such that $\sz{V} = k + 1$. Now, we remove a vertex $v$ with no incoming
    edges from $G_{2}$. When we remove this vertex we also remove every edge adjacent (i.e., incoming)on this vertex. Now, we have the
    graph $G_{2} - v$ which has $\sz{V} = k$. Since removing an edge cannot form cycles, we have that this new graph is also a
    DAG. Therefore, this graph has a topological ordering.

    Now, we consider the topological ordering of $G_{2} - v$ that is $\{v_{1}, \dots, v_{k}\}$. To this ordering we append the removed
    vertex $v$ at the beginning. So, the new ordering is $v, v_{1}, \dots, v_{k}$. This is a valid ordering since $v$ has no incoming
    edges, so the condition that for every edge $(v_{i}, v_{j})$ we have $i < j$ is met.

    This concludes the proof
\end{proof}

\begin{proof}
    We will prove that if the graph $G$ has a topological order, then $G$ is a DAG.

    We assume that $G$ has a topological order. That is, there exists an ordering $\{v_{1}, \dots, v_{k}\}$ of the vertices of $G$ such
    that for every edge $(v_{i}, v_{j})$ we have $i < j$.

    Now assume, for the sake of contradiction, that $G$ is not a DAG. That is, there is a directed cycle 
    $C$ in $G$. Let $v_{i} \in C$ be the lowest indexed vertex in $C$ and $v_{j} \in C$ be the 
    highest indexed vertex in $C$. This implies that there is an edge $(v_{j}, v_{i})$ such that $i < j$, a contradiction.

    Therefore, $G$ is a DAG as needed.
\end{proof}


\newpage
\section{Paper Review}
\subsection{Summary}
This paper investigates the extent to which a neural network architecture alone, without learning any weight parameters, can encode 
solutions to a given task. The idea is that there are certain animals that are already born with innate knowledge. Therefore, 
it could be possible to create a neural network with a similar property of having innate knowledge. The way to do this is by creating
a very simple set of neural networks and allowing it to "evolve" into a network with this "innate" knowledge. Then, the top performing
neural networks, picked by a measure of performance and simplicity, are picked to evolve further. Moreover, this is done
with a very simple algorithm that allows only three operations: insert node, add connection, change activation. The strength of this
type of architecture search is that one can find a good network using this method and then train it to get even better results. This is
proven by the fact that the weights in the "evolution" are randomly selected from a uniform distribution and shared between all the 
networks in the generation (iteration of evolution).
However, it is not possible to match very researched and tuned problems such as a trained CNN on the MNIST dataset.

\subsection{Personal takeaway}
I personally think this paper is a great example of what is meant by "think outside of the box", instead of trying to go deeper into
a certain rabit hole of Deep Learning, it proposes a novel idea for architecture search. Clearly there is a huge world of possibilities
that have this paper as factor in them. Perhaps combining these methods or extending them somehow, there will no longer be a need to 
think about architectures of machine learning models, and the hyperparameters will simply become parameters (and get replaced by new 
hyperparameters).

\end{document}

