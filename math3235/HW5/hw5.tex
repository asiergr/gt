\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{parskip}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\ra}{\rightarrow}
\newcommand{\partiald}[2]{\frac{\partial #1}{\partial #2}}

% Prob theory specific
\renewcommand{\P}[1]{\mathbb{P}(#1)}
\newcommand{\probspace}{(\Omega, \mathcal{F}, \mathbb{P})}
\newcommand{\Image}[1]{\text{Im}\, #1}
\newcommand{\E}[1]{\mathbb{E}(#1)}
\newcommand{\deriv}[2]{\frac{d #1}{d#2}}

\DeclareMathOperator{\area}{area}

\title{HW4}
\author{Asier Garcia Ruiz }
\date{August 2021}

\begin{document}
\maketitle

\section*{Exercises}
\subsection*{6.14}
Let the pair $(X,Y)$ of random variables have joint distribution function
$F(x,y)$. Prove that
\[\P{a<X \leq b, c<Y\leq d} = F(b,d) + F(a,c) - F(a,d) - F(b,c)\]
for any $a,b,c,d \in \R$ such that $a < b$ and $c < d$.

\begin{proof}
    We know that $\P{a < X \leq b} = F(b) - F(a)$. Hence, we can write
    \begin{align*}
        \P{a<X \leq b, c<Y\leq d} & = \P{X \leq b, c \leq Y \leq d} - \P{X \leq a, c \leq Y \leq d}, \\
                                  & = \P{X \leq b, Y \leq d} + \P{X \leq a, Y \leq c}                \\
                                  & \qquad - \P{X \leq a, Y \leq d} - \P{X \leq b, Y \leq c},        \\
                                  & = F(b,d) + F(a,c) - F(a,d) - F(b,c).
    \end{align*}
\end{proof}

\subsection*{6.26}
Random variables $X$ and $Y$ have joint density function
\begin{equation*}
    f(x,y) = \begin{cases}
        e^{-x-y} & \text{if} \ x,y > 0, \\
        0        & \text{otherwise}.
    \end{cases}
\end{equation*}
Find $\P{X+Y \leq 1}$ and $\P{X>Y}$.

\begin{proof}
    For the first we can write
    \begin{align*}
        \P{X + Y \leq 1} & = \int_0^1 \int_0^{1-x} e^{-x-y} \ dydx,  \\
                         & = \int_0^1 -e^{-x-y} \big |_0^{1-x} \ dx, \\
                         & = \int_0^1 -e^{-1} + e^{-x} \ dx,         \\
                         & = -e^{-1}x - e^{-x} \big |_0^1,           \\
                         & = -e^{-1} - e^{-1} - 0 + 1,               \\
                         & = 1 - 2e^{-1}.
    \end{align*}

    For the second we can write
    \begin{align*}
        \P{X > Y} & = \int_0^\infty \int_0^y e^{-x-y} \ dxdy,                                   \\
                  & = \int_0^\infty -e^{-x-y} \big |_0^y,                                       \\
                  & = \int_0^\infty -e^{-2y} + e^{-y} \ dy,                                     \\
                  & = \lim_{A\rightarrow \infty}[\frac{1}{2}e^{-2y} - e^{-y}]_0^A,              \\
                  & = \lim_{A\rightarrow \infty} \frac{1}{2}e^{-2A} - e^{-A} - \frac{1}{2} + 1, \\
                  & = 0 - 0 -\frac{1}{2} + 1,                                                   \\
                  & = \frac{1}{2}.
    \end{align*}
\end{proof}

\subsection*{6.36}
Random variables $X,Y$ and $Z$ have joint density function
\begin{equation*}
    f(x,y,z) = \begin{cases}
        8xyz & \text{if} \ 0 < x,y,z < 1, \\
        0    & \text{otherwise}.
    \end{cases}
\end{equation*}
Are $X,Y$ and $Z$ independent? Find $\P{X>Y}$ and $\P{Y > Z}$.

\begin{proof}
    Clearly $X, Y$ and $Z$ are independent since we can rewrite
    $f(x,y,z) = g(x)h(y)k(z)$ where $g(x) = 8x$, $h(y) = y$, and
    $k(z) = z$ when $0 < x,y,z < 1$, and $g(x)=h(y)=k(z) = 0$
    otherwise.

    Now to find $\P{X > Y}$ we can write
    \begin{align*}
        \P{X > Y} & = \int_0^1 \int_0^x \int 0^1 8xyz \ dzdydx,   \\
                  & = \int_0^1 \int_0^x 4xyz^2 \big |_0^1 \ dydx, \\
                  & = \int_0^1 \int_0^x 4xy \ dydx,               \\
                  & = \int_0^1 2xy^2 \big |_0^x \ dx,             \\
                  & = \int_0^1 2x^3 \ dx,                         \\
                  & = \frac{2}{4}x^4 \big |_0^1,                  \\
                  & = \frac{1}{2}.
    \end{align*}

    To find $\P{Y > Z}$ we can write
    \begin{align*}
        \P{Y > Z} & = \int_0^1 \int_0^y \int 0^1 8xyz \ dxdzdy,   \\
                  & = \int_0^1 \int_0^y 4zyx^2 \big |_0^1 \ dzdy, \\
                  & = \int_0^1 \int_0^y 4zy \ dzdy,               \\
                  & = \int_0^1 2yz^2 \big |_0^y \ dy,             \\
                  & = \int_0^1 2y^3 \ dy,                         \\
                  & = \frac{2}{4}y^4 \big |_0^1,                  \\
                  & = \frac{1}{2}.
    \end{align*}
\end{proof}

\subsection*{6.45}
If $X$ and $Y$ are independent random variables having the $\chi^2$ distribution
with $m$ and $n$ degrees of freedom, respectively, prove that $X+Y$ has the
$\chi^2$ distribution with $m+n$ degrees of freedom.

\begin{proof}
    Using the convolution formula we can write
    \begin{align*}
        f_Z(z) & = \int_{-\infty}^\infty f_X(x)f_Y(z-x) \ dx,                           \\
               & = \int_0^z
        \frac{1}{2\Gamma(\frac{1}{2}m)}\left(\frac{1}{2}x\right)^{\frac{1}{2}m-1}
        e^{-\frac{1}{2}x}\frac{1}{2\Gamma(\frac{1}{2}n)}
        \left(\frac{1}{2}(z-x)\right)^{\frac{1}{2}n-1}e^{-\frac{1}{2}(z-x)} \ dx,       \\
               & = \int_0^z
        \frac{1}{2\Gamma(\frac{1}{2}m)}\frac{1}{2\Gamma(\frac{1}{2}n)}
        \left(\frac{1}{2}x\right)^{\frac{1}{2}m-1}\left(\frac{1}{2}(z-x)\right)^{\frac{1}{2}n-1}
        e^{-\frac{1}{2}z} \ dx,                                                         \\
               & = \frac{1}{4\Gamma(\frac{1}{2}(m + n))}
        \frac{2}{2^{\frac{1}{2}(m+n)-1}} \int_0^z
        x^{\frac{1}{2}m-1}(z-x)^{\frac{1}{2}n-1}e^{-\frac{1}{2}x} \ dx,                 \\
               & =\frac{1}{2\Gamma(\frac{1}{2}(m + n))}\frac{1}{2^{\frac{1}{2}(m+n)-1}}
        \left(\frac{1}{2}z\right)^{\frac{1}{2}(m+n)-1}e^{-\frac{1}{2}z}.
    \end{align*}
    Which is nothing but the $\chi^2$ distribution with $m+n$ degrees
    of freedom.
\end{proof}

\subsection*{6.55}
Let $X$ and $Y$ be random variables with joint density function
\begin{equation*}
    f(x,y) = \begin{cases}
        \frac{1}{4}e^{-\frac{1}{2}(x+y)} & \text{if} \ x,y>0, \\
        0                                & \text{otherwise.}
    \end{cases}
\end{equation*}
Show that the joint density function of $U=\frac{1}{2}(X-Y)$ and
$V=Y$ is
\begin{equation*}
    f_{U,V}(u,v) = \begin{cases}
        \frac{1}{2}e^{-u-v} & \text{if} (u,v) \in A, \\
        0                   & \text{otherwise}.
    \end{cases}
\end{equation*}

\begin{proof}
    The mapping $T$ is given by $T(x,y) = (u,v) = (\frac{1}{2}(x-y), y)$. We can
    see that $T$ is a bijection from $D=\{(x,y): x,y > 0\}$ to
    ${S = \{(u,v): u \in \R, y>0\}}$. It also has an inverse
    $T_{-1}(u,v) = (x,y) = (2u-v, v)$. The Jacobian of $T^{-1}$ is
    \begin{equation*}
        \begin{vmatrix}
            \partiald{x}{u} & \partiald{x}{v} \\
            \partiald{y}{u} & \partiald{y}{v}
        \end{vmatrix} =
        \begin{vmatrix}
            2 & -1 \\
            0 & 1
        \end{vmatrix} = 2.
    \end{equation*}
    Hence, we have that $U,V$ have joint density function
    \begin{align*}
        f_{U,V}(u,v) & = \frac{1}{4}f_{X,Y}(x(u,v),y(u,v))|J(u,v)|
                     & \text{if} \ (u,v) \in A,                    \\
                     & = \frac{1}{4}e^{-\frac{1}{2}(2u+v+v)}|2|
                     & \text{if} \ (u,v) \in A,                    \\
                     & = \frac{1}{2}e^{-u-v}
                     & \text{if} \ (u,v) \in A,                    \\
    \end{align*}
    and $f_{U,V} = 0$ if $(u,v) \not \in A$, where $A = \R \times (0,\infty)$.
\end{proof}

\subsection*{6.61}
Let $X$, and $Y$ be independent random variables, each having the exponential
distribution with parameter $\lambda$. Find the joint density function of
$X$ and $X+Y$, and deduce that the conditional density function of $X$,
given that $X+Y=a$, in uniform on the interval $(0,a)$ for each $a>0$.
In other words, the knowledge that $X+Y = a$ provides no useful clue
about the position of $X$ in the interval $(0,a)$.

\begin{proof}
    We will start by finding the density function of $Z=X+Y$
\end{proof}

\subsection*{6.70}
Let the pair $(X,Y)$ be uniformly distributed on the unit disc, so that
\begin{equation*}
    f_{X,Y}(x,y) = \begin{cases}
        \pi^{-1} & \text{if} \ x^2 + y^2 \leq 1, \\
        0        & \text{otherwise}.
    \end{cases}
\end{equation*}
Find $\E{\sqrt{X^2 + Y^2}}$ and $\E{X^2 + Y^2}$.

\begin{proof}
    Let $D = \{(x,y): x^2 + y^2 \leq 1\}$. We can now write
    \begin{align*}
        \E{\sqrt{X^2+Y^2}} & = \int\int_D \pi^{-1}\sqrt{x^2+y^2} \ dA,        \\
        \intertext{changing to polar coordinates for simplicity,}
                           & = \pi^{-1}\int_0^{2\pi}\int_0^1 r^2 \ drd\theta, \\
                           & = \pi^{-1}\int_0^{2\pi}\frac{1}{3} \ d\theta,    \\
                           & = \frac{1}{3\pi}2\pi = \frac{2}{3}.
    \end{align*}

    Using the same $D$ we can write
    \begin{align*}
        \E{\sqrt{X^2+Y^2}} & = \int\int_D \pi^{-1}(x^2+y^2) \ dA,             \\
        \intertext{changing to polar coordinates for simplicity,}
                           & = \pi^{-1}\int_0^{2\pi}\int_0^1 r^3 \ drd\theta, \\
                           & = \pi^{-1}\int_0^{2\pi}\frac{1}{4} \ d\theta,    \\
                           & = \frac{1}{4\pi}2\pi = \frac{1}{2}.
    \end{align*}
\end{proof}


\section*{Problems}
\subsection*{6}
Let $X_1, X_2,...,X_n$ be independent random variables, each having
distribution function $F$ and density function $f$. Find the distribution
function of $U$ and the density functions of $U$ and $V$, where
$U = \min\{X_1,X_2,...,X_n\}$ and $V = \max\{X_1, X_2,...,X_n\}$.
Show that the joint density function of $U$ and $V$ is
\begin{equation*}
    f_{U,V}(u,v) = n(n-1)f(u)f(v)[F(v)-F(u)]^{n-2} \ \text{if} \ u < v.
\end{equation*}

\begin{proof}
    We will begin by finding the distribution function of $U$.
    We can write
    \begin{align*}
        F_U(u) & = \P{U \leq u},                                \\
               & = \P{\min\{X_1,X_2,\dots,X_n\} \leq u},        \\
               & = 1 - \P{\max\{X_1,X_2,\dots,X_n > u\}},       \\
               & = 1 - \P{X_1 > u, X_2 > u,\dots,X_n > u},      \\
        \intertext{by independence,}
               & = 1 -[\P{X_1 > u}\P{X_2 > u}\dots\P{X_n} > u], \\
               & = 1 - [1 - F(u)]^n.
    \end{align*}

    Similarly, the distribution function of $V$ is
    \begin{align*}
        F_V(v) & = \P{V \leq v},                                    \\
               & = \P{\max\{X_1,X_2,\dots,X_n\} \leq v},            \\
               & = \P{X_1 \leq v, X_2 \leq v,\dots,X_n \leq v},     \\
        \intertext{by independece,}
               & = \P{X_1 \leq v}\P{X_2 \leq v}\dots\P{X_n \leq v}, \\
               & = F(v)^n
    \end{align*}

    Now, to find the density function of $U$ and $V$ we will first
    find the joint distribution function
    \begin{align*}
        F_{U,V}(u,v) & = \P{U \leq u, V \leq v}, \\
                     & = \dots,                  \\
                     & = [F(v) - F(u)]^n
    \end{align*}
    Now, to find the density function we can simply take derivatives
    \begin{equation*}
        \frac{\partial^2}{\partial u \partial v}[F(v) - F(u)]^n
        = n(n-1)f(u)f(v)[F(v)-F(u)]^{n-2} \ \text{if} \ u < v.
    \end{equation*}
\end{proof}

\subsection*{20}
Let $X$ and $Y$ be random variables with the vector $(X,Y)$ uniformly
distributed on the region $R= \{(x,y): 0<y<x<1\}$. Write down the joint
probability density function of $(X,Y)$. Find $\P{X+Y < 1}$.

\begin{proof}
    Because the vector $(X,Y)$ is uniformly distributed in $R$ we know
    the density function is of the form
    \begin{equation*}
        f(x,y) = \begin{cases}
            c & \text{if} \ (x,y) \in R, \\
            0 & \text{otherwise}.
        \end{cases}
    \end{equation*}
    Where $c$ is some constant. We know that for some $E\subset \R^2$ we can
    write
    \begin{equation*}
        \P{(X,Y) \in E} = \int\int_{E} f_{X,Y}(x,y)
        = \int\int_{E\cap R} c = c*\area(E\cap R)\ dxdy
    \end{equation*}
    Now, since $f(x,y)$ is a joint density function.
    \begin{align*}
        1 = \P{(X,Y) \in \R^2} = c*\area(\R^2 \cap R) = c*\area(R).
    \end{align*}
    Since, $\area(R) = \frac{1}{2}$, we have that $c = 2$.

    Now, to find $\P{X+Y < 1}$ we can write
    \begin{align*}
        \P{X + Y < 1} & = \int_0^{1/2}\int_0^x f(x,y) \ dydx
        + \int_{1/2}^{1}\int_0^{1-x} f(x,y) \ dydx,                \\
                      & = \int_0^1\int_0^x \frac{1}{2} \ dydx
        +\int_{1/2}^{1}\int_0^{1-x} 2 \ dydx,            \\ , \\
                      & = \int_0^{1/2} 2 \ dx
        + \int_{1/2}^1 2 \ dx,                      \\
                      & = \left[\frac{1}{4}x^2 \right]_0^{1/2}
        + \left[\frac{1}{2}x - \frac{1}{4}x^2 \right]_{1/2}^1,     \\
                      & = \frac{1}{16} + \frac{1}{2} - \frac{1}{4}
        - \left(\frac{1}{4} - \frac{1}{16}\right),                 \\
                      & = \frac{2}{16} = \frac{1}{8}.
    \end{align*}
\end{proof}



\end{document}