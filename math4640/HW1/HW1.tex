\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{parskip}

\usepackage{listings}

\title{HW1}
\author{Asier Garcia Ruiz }
\date{September 2021}

\begin{document}
    \maketitle
    1. (1 point) If $0 < \epsilon <\epsilon_{mach}$ similar in size, explain why computing 
    $$z = x + \left(\frac{1}{\epsilon}\right)y$$ could be a problem for floating point arithmetic.

    \textit{Answer}: Because $\epsilon < \epsilon_{mach}$ the computer can no longer guarantee
    the precision of $\epsilon$. When we divide by it, not only do we get a very large number
    but also a number that is imprecise, because again the precision cannot be guaranteed.

    \vspace{2mm}
    2. Two alternative series approximations for $e^{-x}$ are
    $$e^{-x} = 1 - x + \frac{1}{2}x^2 - \frac{1}{6}x^3 + ...$$ and 
    $$e^{-x} = \frac{1}{1 + x + x^2/2 + x^3/6 + ...}$$
    Of these two, which approach is more prone to finite-precision floating point rounding errors?
    Explain.
    
    \textit{Answer:} The second approach is more prone to finite-precision floating point rounding
    errors. This is because in the second approach, we are dividing $1$ by a possibly very large
    number, meaning that the result $\epsilon$ could be such that $\epsilon < \epsilon_{mach}$. This
    implies a loss of precision on the value of $e^{-x}$.

    \vspace{1cm}
    3. Write  a  Gaussian  elimination  code  (no  pivoting)  that  solves  the  linear  system
    $$\bm{A}=\begin{pmatrix}
        1 & 1/2 &  1/3 \\
        1/2 & 1/3 & 1/4 \\
        1/3 & 1/4 & 1/5 \\
    \end{pmatrix}
    \qquad \bm{b} = \begin{pmatrix}
        7/6 \\ 5/6\\ 13/20
    \end{pmatrix}$$
    Include a listing of your code and your answerx.  What is the residual $r$.

    \textit{Answer:} The code used is
    \begin{lstlisting}[language=Python]

    import numpy as np

    def gaussian_elimination(A: np.array) -> Tuple[np.array, np.array]:
        n = A.shape[0]

        for k in range(0, n - 1): #Loop over cols
            assert A[k][k]
            for i in range(k + 1, n): # compute multipliers
                A[i][k] = A[i][k] / A[k][k]
            for j in range(k + 1, n):
                for i in range(k + 1, n):
                    A[i][j] = A[i][j] - A[i][k]*A[k][j]
        L = A - np.triu(A) + np.eye(n,n)
        return L, np.triu(A)

    def forward_substitution(L: np.array, b: np.array) -> np.array:
        n = L.shape[0]
        x = np.zeros((n, 1))
        for j in range(0, n):
            assert L[j][j]
            x[j] = b[j] / L[j][j]
            for i in range(j + 1, n):
                b[i] = b[i] - L[i][j] * x[j]
        return x

    def backward_substitution(U: np.array, y: np.array) -> np.array:
        n = U.shape[0]
        x = np.zeros((n, 1))
        for j in range(n - 1, -1, -1):
            assert U[j][j]
            x[j] = y[j] / U[j][j]
            for i in range(0, j):
                y[i] = y[i] - U[i][j] * x[j]
        return x

    def main():
        A = np.array([[1, 1/2, 1/3],
                        [1/2, 1/3, 1/4],
                        [1/3, 1/4, 1/5]])
        b = np.array([7/6, 5/6, 13/20])
        L, U = gaussian_elimination(A)
        y = forward_substitution(L, b)
        x = backward_substitution(U, y)
    \end{lstlisting}

    For the given system, the result is
    $$\hat{x} =
    \begin{pmatrix}
        -7.77156117*10^{-16} \\ 1 \\ 2
    \end{pmatrix}$$
    We can calculate the residual
    $$||r|| = ||\bm{b} - \bm{A}\hat{x}|| \approx ||[0\ -0.58333333\ -0.63888889]^T||
    \approx 0.86513397$$

    It is easy to check the true answer is $\vec{x} = [0\ 1\ 2]^T$.

    
    \vspace{0.5cm}
    4. (a) (1 point) Use Gaussian elimination (no pivoting) to solve the linear system $\bm{A}\bm{x}=\bm{b}$
    with $$\bm{A}=\begin{pmatrix}
        0.0001 & 1 \\
        1 & 1 \\
    \end{pmatrix} \qquad \bm{b} = \begin{pmatrix}
        1 \\ 2
    \end{pmatrix}$$
    with three digit decimal (base-10) arithmetic (by hand).  You can assume the machine chops.

    \vspace{0.5cm}
    \textit{Answer:} Because our machine approximates to three decimal digits, matrix $\bm{A}$
    is interpreted as $\bm{A} = \begin{psmallmatrix}
        0 & 1 \\
        1 & 1 \\
    \end{psmallmatrix}$
    This means that one of pivots is 0 for the Gaussian elimination. Since pivotting is not
    allowed, we are done with our elimination. Fortunately, $\bm{A}$ can be solved by
    forward substitution. Hence, we get that $x_1 = 1$ and thus $x_2 + x_1 = 2$ resolves to
    $x_2 = 1$. Thus, $\vec{x} = [1,1]^T$

    \vspace{0.5cm}
    (b) Repeat with partial pivoting. Comment on your results.

    \textit{Answer:} Again, since the machine approximates to three decimal digits,
    the matrix is stored and operated as
    $\bm{A} = \begin{psmallmatrix}
        0 & 1 \\
        1 & 1 \\
    \end{psmallmatrix}$
    Now, with partial pivoting we swap row 1 and row 2 to get the new system
    $$\bm{A}=\begin{pmatrix}
        1 & 1 \\
        0 & 1 \\
    \end{pmatrix} \qquad \bm{b} = \begin{pmatrix}
        2 \\ 1
    \end{pmatrix}$$
    Since we have an upper triangular matrix, we can solve this by backward substitution.
    We get that $x_2 = 1$ and thus $x_1 = 1$. Hence, $\vec{x} = [1,1]^T$.

    To comment on these results. The equality of the result for no pivoting and
    partial pivoting has to do with the fact that the machine chops to 3 decimal digits.
    This means that the $a_{11} = 0.0001$ entry of $\bm{A}$ is interpreted as $0$
    every time. The residual $||\vec{r}|| = 0.0001$ which is still accurate.
    To summarise, the results are the same because of the internal storage
    limitations (3 digit decimal) of the machine when storing $\bm{A}$.

    \vspace{1cm}
    5. Use a computer to determine the condition number of $\bm{A}$ from problem 4 
    (you can use full precision, e.g. double precision).  Explain how you coded it up 
    (I donâ€™t want tosee the code itself).
    
    \vspace{0.5cm}
    \textit{Answer:} \\
    (a) The condition number for $\bm{A}$ is $\approx 2.6184$.
    because this is a $2\times 2$ matrix the inverse $\bm{A}^{-1}$ can be trivially
    calculated in constant time. Hence, the condition number calculated by
    $||\bm{A}||*||\bm{A}^{-1}||$. To find these norms, because they are subordinate
    to the vector norm, we simply generate a constant amount of random vectors and 
    pick the maximal $\frac{||\bm{A}\bm{x}||}{||\bm{x}||}$. \\
    (b) Do you consider this a good or bad condition number?
    Explain your reasoning.

    \vspace{0.5cm}
    I would consider this a good condition number. This is because it is only $\approx 2.5$
    times larger than the condition number for the identity matrix. Considering that the
    upper bound for this value is infinity (i.e it is unbounded), this is a pretty good value.

    \vspace{1cm}
    (c) f the condition number is bad, how could you change $\bm{A}$ to make it better? If the 
    condition number is good, how could you change $\bm{A}$ to make it worse (not that one 
    would want to do this)?

    \vspace{0.5cm}
    \textit{Answer:} To make this value worse we could make $a_{11} = 1$ or make 
    it very close to $1$. Because we know singular matrices have $\infty$ as their condition
    number, this would raise it. 
\end{document}